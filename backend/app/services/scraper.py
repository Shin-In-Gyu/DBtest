from bs4 import BeautifulSoup
import httpx
from urllib.parse import urljoin

async def scrape_notice_content(url: str):
    """
    [2026-01-08 ìˆ˜ì •]
    ì œê³µëœ ìŠ¤í¬ë¦°ìƒ· êµ¬ì¡°ì— ë§ì¶° ì œëª©(.tblw_subj), ë³¸ë¬¸(.tbl_view), íŒŒì¼(.wri_area.file)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    print(f"   â–¶ [ì ‘ì† ì‹œë„] {url}")
    
    try:
        async with httpx.AsyncClient(verify=False) as client:
            response = await client.get(url)
            response.raise_for_status()
    except Exception as e:
        print(f"   âŒ [ì ‘ì† ì‹¤íŒ¨] {e}")
        return {"title": "", "texts": [], "images": [], "files": []}
    
    soup = BeautifulSoup(response.text, 'html.parser')

    data = {
        "title": "",
        "date": "",
        "texts": [],
        "images": [],
        "files": []
    }

    # -------------------------------------------------------
    # 1. ì œëª© ì¶”ì¶œ (í•µì‹¬ ìˆ˜ì • ì‚¬í•­)
    # ìŠ¤í¬ë¦°ìƒ· ê²½ë¡œ: .thead.view -> ul -> li -> .inner_txt -> .tblw_subj
    # -------------------------------------------------------
    title_tag = soup.select_one('.tblw_subj')
    
    if not title_tag:
        # í˜¹ì‹œ ëª¨ë¥¼ ì˜ˆë¹„ìš© (ê¸°ì¡´ ê°•ë‚¨ëŒ€ íŒ¨í„´)
        title_tag = soup.select_one('.subject') or soup.select_one('#contentTit')

    if title_tag:
        # ì œëª© ì•ˆì— [ì·¨ì—…] ê°™ì€ ë±ƒì§€ê°€ spanìœ¼ë¡œ ë“¤ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ í…ìŠ¤íŠ¸ë§Œ ê¹”ë”í•˜ê²Œ ê°€ì ¸ì˜´
        data["title"] = title_tag.get_text(strip=True)
        print(f"   âœ… [ì œëª© ë°œê²¬] {data['title']}")
    else:
        print("   âš ï¸ [ì œëª© ì°¾ê¸° ì‹¤íŒ¨] HTML êµ¬ì¡°ê°€ ë˜ ë‹¤ë¥¸ íŒ¨í„´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    # 1-2. ë‚ ì§œ ì¶”ì¶œ
    # ìŠ¤í¬ë¦°ìƒ· êµ¬ì¡°: .tblw_date -> span -> (<span class="hide_txt">ë“±ë¡ë‚ ì§œ</span>) + "ë‚ ì§œí…ìŠ¤íŠ¸"
    # -------------------------------------------------------
    date_text = ""
    date_tag = soup.select_one('.tblw_date')

    if date_tag:
        for span in date_tag.find_all('span'):
            if "ì¡°íšŒìˆ˜" in span.get_text():
                span.decompose()  # DOMì—ì„œ ì¡°íšŒìˆ˜ ì˜ì—­ ì‚­ì œ
        # "ë“±ë¡ë‚ ì§œ"ë¼ê³  ì íŒ ìˆ¨ê²¨ì§„ ë¼ë²¨(<span class="hide_txt">)ì„ ì°¾ìŠµë‹ˆë‹¤.
        label = date_tag.select_one('.hide_txt')
        
        # ë¼ë²¨ì´ ìˆìœ¼ë©´ DOMì—ì„œ ì•„ì˜ˆ ì‚­ì œ(decompose)í•´ë²„ë¦½ë‹ˆë‹¤. 
        # ê·¸ë˜ì•¼ ë‚˜ì¤‘ì— get_textí•  ë•Œ "ë“±ë¡ë‚ ì§œ"ë¼ëŠ” ê¸€ìê°€ ì„ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
        if label:
            label.decompose()
        
        # ë¼ë²¨ì„ ì§€ìš´ ìƒíƒœì—ì„œ ë‚¨ì€ í…ìŠ¤íŠ¸(ìˆœìˆ˜ ë‚ ì§œ)ë§Œ ê¹”ë”í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        date_text = date_tag.get_text(strip=True)
        print(f"   ğŸ“… [ë‚ ì§œ ë°œê²¬] {date_text}")
    else:
        # í˜¹ì‹œ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì˜ˆë¹„ ë¡œì§ (í•„ìš”ì‹œ ì¶”ê°€)
        print("   âš ï¸ [ë‚ ì§œ] ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ì¶”ì¶œí•œ ë‚ ì§œë¥¼ ë”•ì…”ë„ˆë¦¬ì— ë‹´ìŠµë‹ˆë‹¤.
    data["date"] = date_text

    # -------------------------------------------------------
    # 2. ì²¨ë¶€íŒŒì¼ ì¶”ì¶œ
    # ìŠ¤í¬ë¦°ìƒ· ê²½ë¡œ: .wri_area.file -> a.link_file
    # -------------------------------------------------------
    # ë³¸ë¬¸ê³¼ ë³„ë„ì˜ ì˜ì—­ì— ìˆìœ¼ë¯€ë¡œ ì „ì²´ ë¬¸ì„œì—ì„œ í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    file_links = soup.select('.wri_area.file a.link_file')
    
    if file_links:
        print(f"   ğŸ“ [ì²¨ë¶€íŒŒì¼] {len(file_links)}ê°œ ë°œê²¬")
        for a in file_links:
            f_name = a.get_text(strip=True)
            f_link = a.get('href')
            if f_link:
                # view_image.do ê°™ì€ ì´ë¯¸ì§€ ë³´ê¸° ë§í¬ê°€ ì•„ë‹ˆë¼ download.do ë§í¬ë§Œ ê°€ì ¸ì˜¤ë„ë¡ í•„í„°ë§ ê°€ëŠ¥
                # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ë‹¤ ê°€ì ¸ì˜µë‹ˆë‹¤.
                full_url = urljoin(url, f_link)
                data["files"].append({
                    "name": f_name,
                    "url": full_url
                })
    

    

    # -------------------------------------------------------
    # 3. ë³¸ë¬¸ í…ìŠ¤íŠ¸ & ì´ë¯¸ì§€ ì¶”ì¶œ
    # ìŠ¤í¬ë¦°ìƒ· ê²½ë¡œ: .tbody -> ... -> .tbl_view
    # -------------------------------------------------------
    content_div = soup.select_one('.tbl_view')
    
    if content_div:
        # (1) ì´ë¯¸ì§€ ì¶”ì¶œ (ë³¸ë¬¸ ë‚´ ì‚½ì…ëœ ì´ë¯¸ì§€)
        imgs = content_div.find_all('img')
        for img in imgs:
            src = img.get('src')
            if src:
                data["images"].append(urljoin(url, src))
        
        # (2) í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # ìŠ¤í¬ë¦°ìƒ·ì— ë³´ë©´ <p> íƒœê·¸ê°€ ë§ìœ¼ë¯€ë¡œ p íƒœê·¸ ë‹¨ìœ„ë¡œ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
        lines = []
        paragraphs = content_div.find_all('p')
        
        if paragraphs:
            for p in paragraphs:
                # display:none ìŠ¤íƒ€ì¼ì´ ìˆëŠ” píƒœê·¸(ìˆ¨ê²¨ì§„ í…ìŠ¤íŠ¸)ëŠ” ì œì™¸í• ì§€ ê²°ì •í•´ì•¼ í•¨
                # ì¼ë‹¨ì€ ë‹¤ ê°€ì ¸ì˜¤ë˜, ë„ˆë¬´ ì§€ì €ë¶„í•˜ë©´ í•„í„°ë§ ì¶”ê°€ í•„ìš”
                text = p.get_text(strip=True)
                if text:
                    lines.append(text)
            data["texts"] = lines
        else:
            # píƒœê·¸ê°€ ì—†ëŠ” ê²½ìš° í†µìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
            text = content_div.get_text("\n", strip=True)
            if text:
                data["texts"] = [text]
        
        if data["texts"]:
            print(f"   ğŸ“ [ë³¸ë¬¸] {len(data['texts'])}ì¤„ ì¶”ì¶œë¨")
    else:
        print("   âŒ [ë³¸ë¬¸ ì˜ì—­(.tbl_view) ì°¾ê¸° ì‹¤íŒ¨]")

    return data