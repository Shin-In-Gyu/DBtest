# app/services/knu_notice_service.py
import json
import html as html_lib
import asyncio
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_

from app.services.ai_service import generate_summary
from app.core.config import get_urls, NOTICE_CONFIGS
from app.core.http import fetch_html
from app.database.models import Notice
from app.services.scraper import scrape_notice_content
from app.core.logger import get_logger
from app.services.notification_service import send_keyword_notifications

logger = get_logger()
SCRAPE_SEMAPHORE = asyncio.Semaphore(3) 
NOTIFICATION_TARGET_CATEGORIES = {"academic", "job", "scholar", "library"}

async def crawl_and_sync_notices(db: AsyncSession, category: str = "univ"):
    config = NOTICE_CONFIGS.get(category)
    if not config:
        return

    site_type = config.get("type", "main_cms") 

    candidates_map = {}
    
    if site_type == "library":
        candidates_map = await _crawl_library_list(category, config)
    else:
        candidates_map = await _crawl_main_cms_list(category)

    if not candidates_map:
        # ë¡œê·¸ ë ˆë²¨ì„ Infoë¡œ ë‚®ì¶° ë¶ˆí•„ìš”í•œ ê±±ì • ë°©ì§€ (ë°ì´í„°ê°€ ì§„ì§œ ì—†ì„ ìˆ˜ë„ ìˆìŒ)
        logger.info(f"â„¹ï¸ [{category}] ì‹ ê·œ ê³µì§€ì‚¬í•­ ì—†ìŒ (ë˜ëŠ” ëª©ë¡ íŒŒì‹± ì‹¤íŒ¨)")
        return

    await _process_candidates(db, category, candidates_map)


async def _crawl_main_cms_list(category: str):
    list_url, info_url, default_seq = get_urls(category)
    if not list_url: return {}

    logger.info(f"ğŸ”„ [{category}] CMS ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    try:
        html_text = await fetch_html(list_url, params={"searchMenuSeq": default_seq})
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        logger.error(f"âŒ [{category}] ëª©ë¡ ì ‘ì† ì‹¤íŒ¨: {e}")
        return {}

    items = soup.select("a.detailLink[data-params]")
    candidates = {}

    for a in items:
        try:
            list_title = a.get_text(" ", strip=True) or a.get("title", "").strip()
            raw_params = html_lib.unescape(a.get("data-params", "")).strip()
            
            try: params = json.loads(raw_params)
            except: 
                try: params = json.loads(raw_params.replace("'", '"'))
                except: continue 

            if not (params.get("encMenuSeq") and params.get("encMenuBoardSeq")): continue

            detail_url = (
                f"{info_url}"
                f"?scrtWrtYn={'true' if params.get('scrtWrtYn') else 'false'}"
                f"&encMenuSeq={params.get('encMenuSeq')}"
                f"&encMenuBoardSeq={params.get('encMenuBoardSeq')}"
            )
            candidates[detail_url] = list_title
        except: continue
    
    return candidates


# --------------------------------------------------------------------------
# [Logic B] ë„ì„œê´€ (lib.kangnam.ac.kr) ëª©ë¡ íŒŒì‹± (ìµœì¢… ìˆ˜ì •)
# --------------------------------------------------------------------------
async def _crawl_library_list(category: str, config: dict):
    domain = config.get("domain")
    endpoint = config.get("list_endpoint") # /Board?n=notice
    full_url = urljoin(domain, endpoint)

    logger.info(f"ğŸ”„ [{category}] ë„ì„œê´€ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({full_url})")
    try:
        html_text = await fetch_html(full_url)
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        logger.error(f"âŒ [{category}] ë„ì„œê´€ ì ‘ì† ì‹¤íŒ¨: {e}")
        return {}

    candidates = {}
    
    # [í•µì‹¬ ë³€ê²½] HTML êµ¬ì¡°(dl, table ë“±) ë¬´ì‹œí•˜ê³  "Board/Detail" ë§í¬ë§Œ ì°¾ìŒ
    # ì‚¬ìš©ìê°€ ì œê³µí•œ URL íŒ¨í„´: /Board/Detail/20251218...
    link_items = soup.select("a[href*='Board/Detail']")
    
    if not link_items:
        # í˜¹ì‹œë‚˜ í•´ì„œ ìƒëŒ€ê²½ë¡œ '../Board/Detail' ë“±ë„ ê³ ë ¤
        link_items = soup.select("a[href*='Detail']")

    if not link_items:
        logger.warning(f"âš ï¸ [{category}] ëª©ë¡ì—ì„œ ìƒì„¸ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    for a in link_items:
        try:
            link_href = a.get("href")
            if not link_href: continue
            # URLì— 'n=notice'ê°€ í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš°(ì˜ˆ: n=free)ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.
            # -----------------------------------------------------------
            if "n=notice" not in link_href:
                continue
            # [ì œëª© ì¶”ì¶œ]
            # ë§í¬ ë‚´ë¶€ì— span(ë‚ ì§œ, ì‘ì„±ì ë“±)ì´ ì„ì—¬ìˆìœ¼ë©´ ì œê±°
            # (soup ê°ì²´ ë³µì‚¬ ë¹„ìš©ì„ ì•„ë¼ê¸° ìœ„í•´ í…ìŠ¤íŠ¸ ì •ì œ ë°©ì‹ ì‚¬ìš©)
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ì „ span íƒœê·¸ë“¤ ì„ì‹œ ì œê±° (DOM ì¡°ì‘ ì£¼ì˜)
            # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ textë§Œ ê°€ì ¸ì˜¨ ë’¤ ì •ì œ
            # ë³´í†µ ë„ì„œê´€ êµ¬ì¡°: <a> Title <span class='mobile-date'>Date</span> </a>
            
            # span íƒœê·¸ë¥¼ ì œì™¸í•œ ì§ê³„ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì€ ë³µì¡í•˜ë¯€ë¡œ
            # ê°„ë‹¨íˆ decompose() ì‚¬ìš© (í˜„ì¬ soupëŠ” ì´ í•¨ìˆ˜ ëë‚˜ë©´ ë²„ë ¤ì§€ë¯€ë¡œ ê´œì°®ìŒ)
            for span in a.select("span"):
                span.decompose()
            
            title = a.get_text(" ", strip=True)

            # ì œëª© ìœ íš¨ì„± ì²´í¬
            if not title or len(title) < 2: 
                continue

            full_detail_url = urljoin(full_url, link_href)
            candidates[full_detail_url] = title
            
        except Exception:
            continue

    return candidates

# ... (ì•„ë˜ _process_candidates ë“±ì€ ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
async def _process_candidates(db: AsyncSession, category: str, candidates_map: dict):
    candidate_urls = list(candidates_map.keys())

    # DB ì¤‘ë³µ ì²´í¬
    try:
        stmt = select(Notice.link).where(
            and_(
                Notice.category == category,
                Notice.link.in_(candidate_urls)
            )
        )
        result = await db.execute(stmt)
        existing_links = set(result.scalars().all())
    except Exception as e:
        logger.error(f"ğŸ”¥ [{category}] DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return

    # ì €ì¥ ëŒ€ìƒ ì„ ë³„
    tasks = []      
    meta_info = []  
    processed_in_this_run = set()

    for url, title in candidates_map.items():
        if url in existing_links: continue
        if url in processed_in_this_run: continue
        processed_in_this_run.add(url)

        tasks.append(safe_scrape_with_semaphore(url))
        meta_info.append({"list_title": title, "detail_url": url, "category": category})

    if not tasks:
        return

    logger.info(f"ğŸš€ [{category}] {len(tasks)}ê°œ ì‹ ê·œ ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘")

    results = await asyncio.gather(*tasks, return_exceptions=True)
    new_notices_buffer = [] 
    
    for i, result in enumerate(results):
        if isinstance(result, Exception) or not result:
            logger.warning(f"âš ï¸ ìƒì„¸ íŒŒì‹± ì‹¤íŒ¨: {meta_info[i]['detail_url']}")
            continue
            
        scraped_data = result
        meta = meta_info[i]
        final_title = scraped_data["title"] if scraped_data["title"] else meta["list_title"]
        
        new_notice = Notice(
            title=final_title,
            link=meta["detail_url"],
            date=scraped_data.get("date"),
            content="\n\n".join(scraped_data["texts"]),
            images=scraped_data["images"],
            files=scraped_data["files"],
            category=meta["category"],
            univ_views=scraped_data.get("univ_views", 0),
            app_views=0
        )
        db.add(new_notice)
        new_notices_buffer.append(new_notice)

    # íŠ¸ëœì­ì…˜ ì»¤ë°‹
    if new_notices_buffer:
        try:
            await db.commit()
            logger.info(f"âœ… [{category}] {len(new_notices_buffer)}ê°œ ì €ì¥ ì™„ë£Œ")
            
            if category in NOTIFICATION_TARGET_CATEGORIES:
                try:
                    await send_keyword_notifications(db, new_notices_buffer)
                except Exception as ne:
                    logger.error(f"âš ï¸ ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {ne}")
                    
        except Exception as e:
            await db.rollback()
            if "UNIQUE constraint" in str(e):
                logger.warning(f"âš ï¸ [{category}] ì¤‘ë³µ ë°ì´í„° ë¬´ì‹œë¨")
            else:
                logger.error(f"ğŸ”¥ DB ì»¤ë°‹ ì‹¤íŒ¨: {e}")

async def safe_scrape_with_semaphore(url: str):
    """ì„¸ë§ˆí¬ì–´ë¥¼ ì´ìš©í•œ ì•ˆì „í•œ ìŠ¤í¬ë˜í•‘"""
    async with SCRAPE_SEMAPHORE:
        await asyncio.sleep(0.5) 
        return await scrape_notice_content(url)

async def get_or_create_summary(db: AsyncSession, notice_id: int) -> str:
    """ê³µì§€ì‚¬í•­ ìš”ì•½ì„ ê°€ì ¸ì˜¤ê±°ë‚˜, ì—†ìœ¼ë©´ ìƒì„±(í•„ìš”ì‹œ ì¬í¬ë¡¤ë§)í•˜ì—¬ ì €ì¥"""
    stmt = select(Notice).where(Notice.id == notice_id)
    result = await db.execute(stmt)
    notice = result.scalars().first()
    
    if not notice:
        raise ValueError("Notice not found")
        
    if notice.summary:
        return notice.summary

    content_to_use = notice.content or ""
    image_list = []
    
    if notice.images:
        try: image_list = json.loads(str(notice.images))
        except: pass

    if len(content_to_use) < 2:
        logger.info(f"ğŸ” [Auto-Rescrape] ID:{notice_id} ë³¸ë¬¸ ë³´ê°• ì‹œë„")
        scraped_data = await scrape_notice_content(notice.link)
        if scraped_data:
            content_to_use = "\n\n".join(scraped_data["texts"])
            image_list = scraped_data["images"]
            notice.content = content_to_use
            notice.images = json.dumps(image_list, ensure_ascii=False)

    summary = await generate_summary(content_to_use, image_list)
    notice.summary = summary
    await db.commit()
    return summary

async def search_notices(db: AsyncSession, category: str, query: str = None, skip: int = 0, limit: int = 20, sort_by: str = "date"):
    """ê³µì§€ì‚¬í•­ ê²€ìƒ‰ ë° ì¡°íšŒ (APIìš©)"""
    stmt = select(Notice)
    if category != "all":
        stmt = stmt.where(Notice.category == category)
    if query:
        stmt = stmt.where(Notice.title.like(f"%{query}%"))  
    if sort_by == "views":
        stmt = stmt.order_by(Notice.univ_views.desc())
    else:
        stmt = stmt.order_by(Notice.date.desc().nulls_last(), Notice.id.desc())
    stmt = stmt.offset(skip).limit(limit)
    result = await db.execute(stmt)
    return result.scalars().all()