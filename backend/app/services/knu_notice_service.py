import json
import html as html_lib
import asyncio
from bs4 import BeautifulSoup
from sqlalchemy.orm import Session
from sqlalchemy import or_

from app.core.config import get_urls 
from app.core.http import fetch_html
from app.database.models import Notice
from app.services.scraper import scrape_notice_content

# [Concurrency] ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
SCRAPE_SEMAPHORE = asyncio.Semaphore(5)

async def crawl_and_sync_notices(db: Session, category: str = "univ"):
    print(f"ğŸ”„ [{category}] ë™ê¸°í™” ì‘ì—… ì‹œì‘...")
    
    list_url, info_url, default_seq = get_urls(category)
    
    # 1. ëª©ë¡ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ì²˜ë¦¬)
    try:
        html_text = await fetch_html(list_url, params={"searchMenuSeq": default_seq})
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        print(f"âŒ [{category}] ëª©ë¡ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
        return

    processed_links = set() 
    tasks = []      
    meta_info = []  

    # 2. ëª©ë¡ íŒŒì‹± ë° Task ìƒì„±
    for a in soup.select("a.detailLink[data-params]"):
        try:
            list_title = a.get_text(" ", strip=True) or a.get("title", "").strip()
            raw = html_lib.unescape(a.get("data-params", "")).strip()

            # JSON íŒŒì‹± ì—ëŸ¬ ì²˜ë¦¬
            try:
                params = json.loads(raw)
            except json.JSONDecodeError:
                # í™‘ë”°ì˜´í‘œ ì²˜ë¦¬ ì‹œë„
                try:
                    params = json.loads(raw.replace("'", '"'))
                except Exception:
                    continue 

            enc_menu_seq = params.get("encMenuSeq")
            enc_menu_board_seq = params.get("encMenuBoardSeq")
            scrt_wrt_yn = params.get("scrtWrtYn", False)

            if not (enc_menu_seq and enc_menu_board_seq):
                continue

            detail_url = (
                f"{info_url}"
                f"?scrtWrtYn={'true' if scrt_wrt_yn else 'false'}"
                f"&encMenuSeq={enc_menu_seq}"
                f"&encMenuBoardSeq={enc_menu_board_seq}"
            )

            if detail_url in processed_links: continue
            processed_links.add(detail_url)

            # DB ì¤‘ë³µ ì²´í¬
            if db.query(Notice).filter(Notice.link == detail_url).first():
                continue

            # Task ì¶”ê°€
            tasks.append(safe_scrape_with_semaphore(detail_url))
            meta_info.append({
                "list_title": list_title,
                "detail_url": detail_url,
                "category": category
            })

        except Exception as e:
            # ëª©ë¡ ì•„ì´í…œ í•˜ë‚˜ ì²˜ë¦¬í•˜ë‹¤ ì—ëŸ¬ë‚˜ë„ ì „ì²´ëŠ” ê³„ì† ì§„í–‰
            print(f"âš ï¸ ì•„ì´í…œ ì „ì²˜ë¦¬ ìŠ¤í‚µ: {e}")
            continue

    if not tasks:
        print(f"ğŸ’¤ [{category}] ìƒˆë¡œìš´ ê³µì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ğŸš€ [{category}] {len(tasks)}ê°œì˜ ê³µì§€ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (Async)...")
    
    # [Concurrency] ë³‘ë ¬ ì‹¤í–‰
    results = await asyncio.gather(*tasks)

    # 3. DB ì €ì¥
    new_count = 0
    
    for i, scraped_data in enumerate(results):
        meta = meta_info[i]
        
        # [Exception Handling] ìŠ¤í¬ë˜í¼ê°€ Noneì„ ë°˜í™˜í–ˆë‹¤ë©´(ì—ëŸ¬ ë°œìƒ) ì €ì¥ ê±´ë„ˆëœ€
        if scraped_data is None:
            # print(f"   Pass: {meta['list_title']} (ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨)")
            continue

        try:
            final_title = scraped_data["title"] if scraped_data["title"] else meta["list_title"]
            content_body = "\n\n".join(scraped_data["texts"])
            images_json = json.dumps(scraped_data["images"], ensure_ascii=False)
            files_json = json.dumps(scraped_data["files"], ensure_ascii=False)
            
            # [Date Fix] ì´ì œ post_dateëŠ” datetime.date ê°ì²´ì„
            post_date = scraped_data.get("date")
            u_views = scraped_data.get("univ_views", 0)

            new_notice = Notice(
                title=final_title,
                link=meta["detail_url"],
                date=post_date,       # Date íƒ€ì… ì»¬ëŸ¼ì— ê°ì²´ ì €ì¥
                content=content_body,
                images=images_json,
                files=files_json,
                category=meta["category"],
                univ_views=u_views,
                app_views=0
            )
            
            db.add(new_notice)
            new_count += 1
            
        except Exception as e:
            print(f"âš ï¸ DB ë§¤í•‘ ì—ëŸ¬ ({meta['list_title']}): {e}")

    try:
        db.commit()
        if new_count > 0:
            print(f"âœ… [{category}] {new_count}ê°œ ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        db.rollback()
        print(f"ğŸ”¥ DB ì»¤ë°‹ ì‹¤íŒ¨: {e}")


async def safe_scrape_with_semaphore(url: str):
    """
    ì„¸ë§ˆí¬ì–´ë¥¼ í†µí•´ ë™ì‹œ ì‹¤í–‰ ì œì–´
    """
    async with SCRAPE_SEMAPHORE:
        return await scrape_notice_content(url)


def search_notices_from_db(
    db: Session, 
    category: str, 
    query: str = None, 
    skip: int = 0, 
    limit: int = 20,
    sort_by: str = "date"
):
    sql = db.query(Notice)
    
    if category != "all":
        sql = sql.filter(Notice.category == category)
    
    if query:
        search_filter = f"%{query}%"
        sql = sql.filter(
            or_(Notice.title.like(search_filter), Notice.content.like(search_filter))
        )
    
    if sort_by == "views":
        # ì¸ê¸°ìˆœ
        sql = sql.order_by((Notice.univ_views + Notice.app_views).desc())
    else:
        # [Date Fix] ë‚ ì§œìˆœ ì •ë ¬ (Date íƒ€ì…ì´ë¯€ë¡œ ì´ì œ 1ì›”ì´ 10ì›”ë³´ë‹¤ ì•ìœ¼ë¡œ ì˜¤ì§€ ì•Šê³  ì •ìƒ ì‘ë™í•¨)
        sql = sql.order_by(Notice.date.desc(), Notice.id.desc())
    
    return sql.offset(skip).limit(limit).all()