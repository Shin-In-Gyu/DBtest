# app/services/knu_notice_service.py
import json
import html as html_lib
import asyncio
from bs4 import BeautifulSoup
from sqlalchemy.orm import Session
from sqlalchemy import or_

from app.core.config import get_urls 
from app.core.http import fetch_html
from app.database.models import Notice
from app.services.scraper import scrape_notice_content
from app.core.logger import get_logger
from app.services.notification_service import send_keyword_notifications

logger = get_logger()
# ë™ì‹œ ì ‘ì† ì œí•œ (ë„ˆë¬´ ë§Žì€ ìš”ì²­ì€ í•™êµ ì„œë²„ IP ì°¨ë‹¨ ì›ì¸ì´ ë¨)
SCRAPE_SEMAPHORE = asyncio.Semaphore(5)
NOTIFICATION_TARGET_CATEGORIES = {"academic", "job", "scholar"}

async def crawl_and_sync_notices(db: Session, category: str = "univ"):
    """
    í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ê³µì§€ì‚¬í•­ ëª©ë¡ì„ ê¸ì–´ì™€ì„œ DBì™€ ë™ê¸°í™”í•©ë‹ˆë‹¤.
    """
    list_url, info_url, default_seq = get_urls(category)
    if not list_url:
        return

    logger.info(f"ðŸ”„ [{category}] ë™ê¸°í™” ì‹œìž‘...")
    
    # 1. ëª©ë¡ HTML ê°€ì ¸ì˜¤ê¸°
    try:
        html_text = await fetch_html(list_url, params={"searchMenuSeq": default_seq})
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        logger.error(f"âŒ [{category}] ëª©ë¡ ì ‘ê·¼ ì‹¤íŒ¨: {e}")
        return

    # 2. íŒŒì‹± ë° ì‹ ê·œ ê²Œì‹œê¸€ íƒìƒ‰
    tasks = []      
    meta_info = []  
    processed_links = set()

    items = soup.select("a.detailLink[data-params]")
    if not items:
        return

    for a in items:
        try:
            # ì œëª© ë° íŒŒë¼ë¯¸í„° ì¶”ì¶œ
            list_title = a.get_text(" ", strip=True) or a.get("title", "").strip()
            raw_params = html_lib.unescape(a.get("data-params", "")).strip()

            # ì•ˆì „í•œ JSON ë¡œë“œ
            params = {}
            try:
                params = json.loads(raw_params)
            except json.JSONDecodeError:
                # ê°€ë” ë”°ì˜´í‘œê°€ ìž˜ëª»ëœ ê²½ìš°ê°€ ìžˆìŒ -> ë‹¨ìˆœ ì¹˜í™˜ ì‹œë„
                try: 
                    params = json.loads(raw_params.replace("'", '"'))
                except: 
                    continue 

            if not (params.get("encMenuSeq") and params.get("encMenuBoardSeq")):
                continue

            # ìƒì„¸ URL ì¡°í•©
            detail_url = (
                f"{info_url}"
                f"?scrtWrtYn={'true' if params.get('scrtWrtYn') else 'false'}"
                f"&encMenuSeq={params.get('encMenuSeq')}"
                f"&encMenuBoardSeq={params.get('encMenuBoardSeq')}"
            )

            # ì¤‘ë³µ ì²´í¬
            if detail_url in processed_links: 
                continue
            processed_links.add(detail_url)
            
            # DBì— ì´ë¯¸ ìžˆëŠ”ì§€ í™•ì¸ (ê°€ë²¼ìš´ ì¿¼ë¦¬)
            if db.query(Notice.id).filter(Notice.link == detail_url).first():
                continue

            # ì‹ ê·œ ê¸€ì´ë©´ ìƒì„¸ ìŠ¤í¬ëž˜í•‘ íƒœìŠ¤í¬ ì¶”ê°€
            tasks.append(safe_scrape_with_semaphore(detail_url))
            meta_info.append({
                "list_title": list_title, 
                "detail_url": detail_url, 
                "category": category
            })

        except Exception:
            continue

    if not tasks:
        return

    logger.info(f"ðŸš€ [{category}] {len(tasks)}ê°œ ì‹ ê·œ ê³µì§€ ìˆ˜ì§‘ ì¤‘...")
    
    # 3. ë¹„ë™ê¸° ë³‘ë ¬ ìŠ¤í¬ëž˜í•‘ ìˆ˜í–‰
    results = await asyncio.gather(*tasks)

    new_notices_buffer = [] 
    success_count = 0
    
    # 4. ê²°ê³¼ ì €ìž¥
    for i, scraped_data in enumerate(results):
        if scraped_data is None: 
            continue
        
        meta = meta_info[i]
        final_title = scraped_data["title"] if scraped_data["title"] else meta["list_title"]
        
        try:
            new_notice = Notice(
                title=final_title,
                link=meta["detail_url"],
                date=scraped_data.get("date"),
                content="\n\n".join(scraped_data["texts"]),
                images=json.dumps(scraped_data["images"], ensure_ascii=False),
                files=json.dumps(scraped_data["files"], ensure_ascii=False),
                category=meta["category"],
                univ_views=scraped_data.get("univ_views", 0),
                app_views=0
            )
            
            db.add(new_notice)
            success_count += 1
            
            new_notices_buffer.append({
                "title": final_title,
                "link": meta["detail_url"],
                "category": meta["category"]
            })
            
        except Exception as e:
            logger.error(f"âš ï¸ DB ë§¤í•‘ ì—ëŸ¬ ({meta['list_title']}): {e}")

    if success_count > 0:
        try:
            db.commit()
            logger.info(f"âœ… [{category}] {success_count}ê°œ ì €ìž¥ ì™„ë£Œ")
            
            # í‚¤ì›Œë“œ ì•Œë¦¼ ë°œì†¡
            if category in NOTIFICATION_TARGET_CATEGORIES:
                await send_keyword_notifications(db, new_notices_buffer)
                
        except Exception as e:
            db.rollback()
            logger.critical(f"ðŸ”¥ DB ì»¤ë°‹ ì‹¤íŒ¨: {e}")

async def safe_scrape_with_semaphore(url: str):
    """
    ì„¸ë§ˆí¬ì–´ë¥¼ ì´ìš©í•´ ë™ì‹œ ì ‘ì† ìˆ˜ë¥¼ ì œí•œí•˜ë©° ìŠ¤í¬ëž˜í•‘í•©ë‹ˆë‹¤.
    """
    async with SCRAPE_SEMAPHORE:
        await asyncio.sleep(0.1) # ì„œë²„ ê³¼ë¶€í•˜ ë°©ì§€ìš© ë¯¸ì„¸ ë”œë ˆì´
        return await scrape_notice_content(url)

def search_notices(db: Session, category: str, query: str = None, skip: int = 0, limit: int = 20, sort_by: str = "date"):
    """
    ê³µì§€ì‚¬í•­ ê²€ìƒ‰ ë° í•„í„°ë§ ì¿¼ë¦¬ ë¹Œë”
    """
    sql = db.query(Notice)
    
    if category != "all":
        sql = sql.filter(Notice.category == category)
        
    if query:
        search_filter = f"%{query}%"
        sql = sql.filter(or_(Notice.title.like(search_filter), Notice.content.like(search_filter)))
    
    if sort_by == "views":
        # í•™êµ ì¡°íšŒìˆ˜ + ì•± ë‚´ ì¡°íšŒìˆ˜ í•©ì‚° ì •ë ¬
        sql = sql.order_by((Notice.univ_views + Notice.app_views).desc())
    else:
        # ìµœì‹ ìˆœ (ë‚ ì§œ -> ID ì—­ìˆœ)
        sql = sql.order_by(Notice.date.desc(), Notice.id.desc())
    
    return sql.offset(skip).limit(limit).all()