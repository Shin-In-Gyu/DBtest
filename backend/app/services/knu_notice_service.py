# app/services/knu_notice_service.py
import json
import html as html_lib
import asyncio
import re
from typing import Optional, List, Dict, Any, Union, cast
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_
from urllib.parse import urljoin, urlparse, parse_qsl, urlencode
from app.services.ai_service import generate_summary
from app.core.config import get_urls, NOTICE_CONFIGS
from app.core.http import fetch_html
from app.database.models import Notice
from app.services.scraper import scrape_notice_content
from app.core.logger import get_logger
from app.services.notification_service import send_keyword_notifications

logger = get_logger()
SCRAPE_SEMAPHORE = asyncio.Semaphore(3) 
NOTIFICATION_TARGET_CATEGORIES = {"academic", "job", "scholar", "event_internal", "event_external"}

async def crawl_and_sync_notices(db: AsyncSession, category: str = "univ"):
    config = NOTICE_CONFIGS.get(category)
    if not config:
        return

    site_type = config.get("type", "main_cms") 

    candidates_map = {}
    
    if site_type == "library":
        # candidates_map = await _crawl_library_list(category, config)
        pass
    elif site_type == "daeple":  # [Fix] ëŒ€í”Œ(ì·¨ì°½ì—…) í•¨ìˆ˜ ì—°ê²°
        # candidates_map = await _crawl_daeple_list(category, config)
        pass
    else:
        candidates_map = await _crawl_main_cms_list(category)

    if not candidates_map:
        logger.info(f"â„¹ï¸ [{category}] ì‹ ê·œ ê³µì§€ì‚¬í•­ ì—†ìŒ (ë˜ëŠ” ëª©ë¡ íŒŒì‹± ì‹¤íŒ¨)")
        return

    await _process_candidates(db, category, candidates_map)


async def _crawl_main_cms_list(category: str):
    list_url, info_url, default_seq = get_urls(category)
    if not list_url: return {}

    logger.info(f"ğŸ”„ [{category}] CMS ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    try:
        params = {"searchMenuSeq": default_seq} if default_seq else {}
        html_text = await fetch_html(list_url, params={"searchMenuSeq": default_seq})
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        logger.error(f"âŒ [{category}] ëª©ë¡ ì ‘ì† ì‹¤íŒ¨: {e}")
        return {}

    items = soup.select("a.detailLink[data-params]")
    candidates = {}

    for a in items:
        try:
            # [Fix] Pylance Error: 'strip' unknown for AttributeValueList
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            text_title = a.get_text(" ", strip=True)
            
            # 2. title ì†ì„± ì•ˆì „í•˜ê²Œ ì¶”ì¶œ (List | str | None ëŒ€ì‘)
            attr_title_val = a.get("title", "")
            if isinstance(attr_title_val, list):
                attr_title_val = " ".join(attr_title_val)
            elif attr_title_val is None:
                attr_title_val = ""
            
            final_attr_title = str(attr_title_val).strip()
            
            # 3. ìµœì¢… ì œëª© ê²°ì •
            list_title = text_title or final_attr_title
            
            # [Fix] Pylance Error: get() returns str | list | None ëŒ€ì‘
            raw_params_val = a.get("data-params", "")
            if isinstance(raw_params_val, list):
                raw_params_val = "".join(raw_params_val)
            elif raw_params_val is None:
                raw_params_val = ""
            
            raw_params = html_lib.unescape(str(raw_params_val)).strip()
            
            try: params = json.loads(raw_params)
            except: 
                try: params = json.loads(raw_params.replace("'", '"'))
                except: continue 

            if not (params.get("encMenuSeq") and params.get("encMenuBoardSeq")): continue

            detail_url = (
                f"{info_url}"
                f"?scrtWrtYn={'true' if params.get('scrtWrtYn') else 'false'}"
                f"&encMenuSeq={params.get('encMenuSeq')}"
                f"&encMenuBoardSeq={params.get('encMenuBoardSeq')}"
            )
            candidates[detail_url] = list_title
        except: continue
    
    return candidates


# --------------------------------------------------------------------------
# [Logic B] ë„ì„œê´€ (lib.kangnam.ac.kr) ëª©ë¡ íŒŒì‹±
# --------------------------------------------------------------------------
async def _crawl_library_list(category: str, config: dict):
    domain = str(config.get("domain", ""))
    endpoint = str(config.get("url_path", "/Board?n=notice"))
    full_url = urljoin(domain, endpoint)

    logger.info(f"ğŸ”„ [{category}] ë„ì„œê´€ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({full_url})")
    try:
        html_text = await fetch_html(full_url)
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        logger.error(f"âŒ [{category}] ë„ì„œê´€ ì ‘ì† ì‹¤íŒ¨: {e}")
        return {}

    candidates = {}
    
    items = soup.select("dl.onroad-board dd")
    
    if not items:
        items = soup.select("a[href*='Board/Detail']")

    if not items:
        logger.warning(f"âš ï¸ [{category}] ëª©ë¡ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    for item in items:
        try:
            a_tag = item.find("a") if item.name == "dd" else item
            if not a_tag: continue

            link_href = a_tag.get("href")
            if not isinstance(link_href, str):
                continue

            if "n=notice" not in link_href:
                continue
            
            import copy
            a_clone = copy.copy(a_tag)
            for tag in a_clone.select("span, i, em"):
                tag.decompose()
            
            title = a_clone.get_text(" ", strip=True)

            if not title or len(title) < 2: 
                continue

            full_detail_url = urljoin(str(domain), link_href)
            candidates[full_detail_url] = title
            
        except Exception:
            continue

    return candidates
# --------------------------------------------------------------------------
# [Logic C] ëŒ€í”Œ (ì·¨ì°½ì—…) ëª©ë¡ íŒŒì‹± (Javascript fnDetail í•´ì„)
# --------------------------------------------------------------------------
# [Logic C] ëŒ€í”Œ (ì·¨ì°½ì—…) ëª©ë¡ íŒŒì‹± (Javascript fnDetail í•´ì„ - ìµœì¢… ìˆ˜ì •íŒ)
from urllib.parse import urlparse, parse_qsl, urlencode # ìƒë‹¨ importì— ì¶”ê°€ í•„ìš”

async def _crawl_daeple_list(category: str, config: dict):
    list_url, info_base_url, _ = get_urls(category)
    if not list_url: return {}

    logger.info(f"ğŸ”„ [{category}] ëŒ€í”Œ ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘... ({list_url})")
    try:
        html_text = await fetch_html(list_url)
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        logger.error(f"âŒ [{category}] ëŒ€í”Œ ì ‘ì† ì‹¤íŒ¨: {e}")
        return {}

    candidates = {}
    
    # [1] ëª©ë¡ URLì—ì„œ í•„ìˆ˜ íŒŒë¼ë¯¸í„°(ë©”ë‰´ì½”ë“œ ë“±) ì¶”ì¶œ
    # ì˜ˆ: CURRENT_MENU_CODE=MENU0067&TOP_MENU_CODE=MENU0067&BD_NO=1
    parsed_list_url = urlparse(list_url)
    base_query_params = dict(parse_qsl(parsed_list_url.query))
    
    # [2] í–‰(Row) ì°¾ê¸°
    rows = soup.select(".bbsBoard tbody tr")
    if not rows: rows = soup.select("table tbody tr")
    if not rows: rows = soup.select("table tr")
        
    logger.info(f"ğŸ” [{category}] ê°ì§€ëœ í–‰ ê°œìˆ˜: {len(rows)}")

    for i, row in enumerate(rows):
        try:
            # ì œëª© ì…€ ì°¾ê¸° (th ë˜ëŠ” td)
            title_cell = (
                row.select_one("th.ellipsis") or 
                row.select_one("td.ellipsis") or 
                row.select_one("td.subject")
            )
            
            if not title_cell:
                # ëª» ì°¾ì•˜ìœ¼ë©´ a íƒœê·¸ê°€ ìˆëŠ” ì²« ë²ˆì§¸ ì…€ ì‹œë„
                for cell in row.find_all(['td', 'th']):
                    if cell.find('a'):
                        title_cell = cell
                        break
            
            if not title_cell: continue
            
            a_tag = title_cell.find("a")
            if not a_tag: continue

            # ë°ì´í„° ì¶”ì¶œ
            title = a_tag.get_text(" ", strip=True)
            href = a_tag.get("href") or a_tag.get("onclick") or ""
            
            if len(title) < 2: continue

            # [3] ìë°”ìŠ¤í¬ë¦½íŠ¸ ì¸ì íŒŒì‹±
            # fnDetail('3862', '', '109414', '1') -> [3862, '', 109414, 1]
            args = re.findall(r"['\"]([^'\"]*)['\"]", str(href))
            
            if len(args) >= 3:
                ntt_sn = args[0]  # 3862
                bbs_id = args[2]  # 109414
                
                # [4] URL ì¡°ë¦½ (í•„ìˆ˜ íŒŒë¼ë¯¸í„° ë³‘í•©)
                # ê¸°ì¡´ q_bbsId -> bbsId ë¡œ ë³€ê²½ (404 í•´ê²° ì‹œë„)
                detail_params = {
                    "bbsId": bbs_id,
                    "nttSn": ntt_sn,
                    **base_query_params # ë¦¬ìŠ¤íŠ¸ì˜ ë©”ë‰´ ì½”ë“œ ë“±ì„ ê·¸ëŒ€ë¡œ ìƒì†
                }
                
                # ì¿¼ë¦¬ìŠ¤íŠ¸ë§ ìƒì„±
                query_string = urlencode(detail_params)
                full_detail_url = f"{info_base_url}?{query_string}"
                
                candidates[full_detail_url] = title
            else:
                if i < 3: 
                    logger.warning(f"âš ï¸ [daeple] ë§í¬ íŒŒì‹± ì‹¤íŒ¨ (Row {i}): {href}")

        except Exception as e:
            logger.error(f"âŒ [daeple] Row {i} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
            continue

    if candidates:
        logger.info(f"âœ… [{category}] ìœ íš¨ ê³µì§€ {len(candidates)}ê°œ ì‹ë³„ë¨")
    else:
        logger.warning(f"âš ï¸ [{category}] í–‰ì€ ì°¾ì•˜ìœ¼ë‚˜ ìœ íš¨í•œ ê³µì§€ ë§í¬ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
    return candidates

async def _process_candidates(db: AsyncSession, category: str, candidates_map: dict):
    candidate_urls = list(candidates_map.keys())
    if not candidate_urls: return

    try:
        stmt = select(Notice.link).where(
            and_(
                Notice.category == category,
                Notice.link.in_(candidate_urls)
            )
        )
        result = await db.execute(stmt)
        existing_links = set(result.scalars().all())
    except Exception as e:
        logger.error(f"ğŸ”¥ [{category}] DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return

    tasks = []      
    meta_info = []  
    processed_in_this_run = set()

    for url, title in candidates_map.items():
        if url in existing_links: continue
        if url in processed_in_this_run: continue
        processed_in_this_run.add(url)

        tasks.append(safe_scrape_with_semaphore(url))
        meta_info.append({"list_title": title, "detail_url": url, "category": category})

    if not tasks:
        return

    logger.info(f"ğŸš€ [{category}] {len(tasks)}ê°œ ì‹ ê·œ ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘")

    results = await asyncio.gather(*tasks, return_exceptions=True)
    new_notices_buffer = [] 
    
    for i, result in enumerate(results):
        # [ìˆ˜ì •] Exception íƒ€ì… ì²´í¬ ê°•í™”
        if isinstance(result, Exception) or not result:
            logger.warning(f"âš ï¸ ìƒì„¸ íŒŒì‹± ì‹¤íŒ¨: {meta_info[i]['detail_url']} | ì‚¬ìœ : {result}")
            continue
        
        scraped_data = cast(Dict[str, Any], result)
        meta = meta_info[i]
        
        # [ë³´ì™„] ì œëª©ì´ í¬ë¡¤ë§ ë°ì´í„°ì— ì—†ìœ¼ë©´ ëª©ë¡ ì œëª© ì‚¬ìš© (Pylance safe)
        scraped_title = scraped_data.get("title")
        final_title = str(scraped_title) if scraped_title else meta["list_title"]
        
        # [ë³´ì™„] ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¡°ì¸
        content_lines = scraped_data.get("texts", [])
        final_content = "\n\n".join(content_lines) if isinstance(content_lines, list) else ""

        new_notice = Notice(
            title=final_title,
            link=meta["detail_url"],
            date=scraped_data.get("date"),
            content=final_content,
            images=scraped_data.get("images", []),
            files=scraped_data.get("files", []),
            category=meta["category"],
            univ_views=scraped_data.get("univ_views", 0),
            app_views=0
        )
        db.add(new_notice)
        new_notices_buffer.append(new_notice)
    if new_notices_buffer:
        try:
            await db.commit()
            logger.info(f"âœ… [{category}] {len(new_notices_buffer)}ê°œ ì €ì¥ ì™„ë£Œ")
            
            if category in NOTIFICATION_TARGET_CATEGORIES:
                try:
                    await send_keyword_notifications(db, new_notices_buffer)
                except Exception as ne:
                    logger.error(f"âš ï¸ ì•Œë¦¼ ë°œì†¡ ì¤‘ ì˜¤ë¥˜: {ne}")
                    
        except Exception as e:
            await db.rollback()
            if "UNIQUE constraint" in str(e):
                logger.warning(f"âš ï¸ [{category}] ì¤‘ë³µ ë°ì´í„° ë¬´ì‹œë¨")
            else:
                logger.error(f"ğŸ”¥ DB ì»¤ë°‹ ì‹¤íŒ¨: {e}")

async def safe_scrape_with_semaphore(url: str):
    async with SCRAPE_SEMAPHORE:
        await asyncio.sleep(0.5) 
        return await scrape_notice_content(url)

async def get_or_create_summary(db: AsyncSession, notice_id: int) -> str:
    stmt = select(Notice).where(Notice.id == notice_id)
    result = await db.execute(stmt)
    notice = result.scalars().first()
    
    if not notice:
        raise ValueError("Notice not found")
        
    if notice.summary:
        return notice.summary

    content_to_use = notice.content or ""
    image_list = []
    
    if notice.images:
        try: image_list = json.loads(str(notice.images))
        except: pass

    if len(content_to_use) < 5:
        logger.info(f"ğŸ” [Auto-Rescrape] ID:{notice_id} ë³¸ë¬¸ ë³´ê°• ì‹œë„")
        scraped_data = await scrape_notice_content(notice.link)
        if scraped_data:
            content_to_use = "\n\n".join(scraped_data.get("texts", []))
            image_list = scraped_data.get("images", [])
            notice.content = content_to_use
            try:
                notice.images = image_list 
            except: pass
            
    summary = await generate_summary(content_to_use, image_list)
    notice.summary = summary
    await db.commit()
    return summary

async def search_notices(
    db: AsyncSession, 
    category: str, 
    query: Optional[str] = None, 
    skip: int = 0, 
    limit: int = 20, 
    sort_by: str = "date"
):
    """ê³µì§€ì‚¬í•­ ê²€ìƒ‰ ë° ì¡°íšŒ (APIìš©)"""
    stmt = select(Notice)
    if category != "all":
        stmt = stmt.where(Notice.category == category)
    if query:
        stmt = stmt.where(Notice.title.like(f"%{query}%"))  
    if sort_by == "views":
        stmt = stmt.order_by(Notice.univ_views.desc())
    else:
        stmt = stmt.order_by(Notice.date.desc().nulls_last(), Notice.id.desc())
    stmt = stmt.offset(skip).limit(limit)
    result = await db.execute(stmt)
    return result.scalars().all()