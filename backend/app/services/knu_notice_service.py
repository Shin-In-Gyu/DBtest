# app/services/knu_notice_service.py
import json
import html as html_lib
import asyncio
import re
from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any, Union, cast
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin, urlparse, parse_qsl, urlencode
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, and_

from app.services.ai_service import generate_summary
from app.core.config import get_urls, NOTICE_CONFIGS
from app.core.http import fetch_html
from app.database.models import Notice
from app.services.scraper import scrape_notice_content
from app.core.logger import get_logger
from app.services.notification_service import send_keyword_notifications

logger = get_logger()
SCRAPE_SEMAPHORE = asyncio.Semaphore(3) 
NOTIFICATION_TARGET_CATEGORIES = {"academic", "job", "scholar", "event_internal", "event_external"}

async def crawl_and_sync_notices(db: AsyncSession, category: str = "univ"):
    config = NOTICE_CONFIGS.get(category)
    if not config:
        return

    site_type = config.get("type", "main_cms") 
    candidates_map: Dict[str, Dict[str, Any]] = {} # [ìˆ˜ì •] êµ¬ì¡° ë³€ê²½: {url: {"title": str, "is_pinned": bool}}
    
    # if site_type == "library":
    #     candidates_map = await _crawl_library_list(category, config)
    # elif site_type == "daeple":
    #     candidates_map = await _crawl_daeple_list(category, config)
    # else:
    candidates_map = await _crawl_main_cms_list(category)

    if not candidates_map:
        logger.info(f"â„¹ï¸ [{category}] ì‹ ê·œ ê³µì§€ì‚¬í•­ ì—†ìŒ (ë˜ëŠ” ëª©ë¡ íŒŒì‹± ì‹¤íŒ¨)")
        return

    await _process_candidates(db, category, candidates_map)


async def _crawl_main_cms_list(category: str) -> Dict[str, Dict[str, Any]]:
    list_url, info_url, default_seq = get_urls(category)
    if not list_url: return {}

    logger.info(f"ğŸ”„ [{category}] CMS ëª©ë¡ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    try:
        html_text = await fetch_html(list_url, params={"searchMenuSeq": default_seq})
        soup = BeautifulSoup(html_text, "html.parser")
    except Exception as e:
        logger.error(f"âŒ [{category}] ëª©ë¡ ì ‘ì† ì‹¤íŒ¨: {e}")
        return {}
    # ê³µì§€ì‚¬í•­ ë§í¬ë“¤ì„ ë¨¼ì € ì°¾ìŠµë‹ˆë‹¤.
    rows = soup.select("div.tbody > ul")
    candidates: Dict[str, Dict[str, Any]] = {}

    for ul in rows:
        try:
            # 1. í•´ë‹¹ í–‰(ul) ì•ˆì— ìƒì„¸ í˜ì´ì§€ ë§í¬(a.detailLink)ê°€ ìˆëŠ”ì§€ í™•ì¸
            a = ul.select_one("a.detailLink[data-params]")
            if not a:
                continue

            # 2. ì œëª© ì¶”ì¶œ (a íƒœê·¸ì˜ í…ìŠ¤íŠ¸ ë˜ëŠ” title ì†ì„±) - í•„ë… ê°ì§€ ì „ì— ë¨¼ì € ì¶”ì¶œ
            text_title = a.get_text(" ", strip=True)
            attr_title = a.get("title", "")
            final_title = text_title or (attr_title if isinstance(attr_title, str) else "")

            # 3. [í•„ë… ê°ì§€] í•´ë‹¹ í–‰(ul) ë‚´ë¶€ì˜ ëª¨ë“  li ìš”ì†Œë¥¼ í™•ì¸í•˜ì—¬ span.ali_a íƒœê·¸ ì°¾ê¸°
            is_pinned = False
            
            # ë°©ë²• 1: ul ë‚´ë¶€ì˜ ëª¨ë“  span.ali_a ê²€ìƒ‰ (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
            pin_elements = ul.select("span.ali_a")
            for pin_elem in pin_elements:
                pin_text = pin_elem.get_text(strip=True)
                if "í•„ë…" in pin_text:
                    is_pinned = True
                    break
            
            # ë°©ë²• 2: ì²« ë²ˆì§¸ li ìš”ì†Œ í™•ì¸ (li.first ë˜ëŠ” li:first-child)
            if not is_pinned:
                first_li = ul.select_one("li.first, li:first-child")
                if first_li:
                    # ì²« ë²ˆì§¸ li ë‚´ë¶€ì˜ span.ali_a í™•ì¸ (ê°€ì¥ ì •í™•)
                    ali_a = first_li.select_one("span.ali_a")
                    if ali_a:
                        ali_a_text = ali_a.get_text(strip=True)
                        if "í•„ë…" in ali_a_text:
                            is_pinned = True
                    # span.ali_aê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ li ì „ì²´ í…ìŠ¤íŠ¸ í™•ì¸
                    if not is_pinned:
                        first_li_text = first_li.get_text(strip=True)
                        if "í•„ë…" in first_li_text:
                            is_pinned = True
            
            # ë°©ë²• 3: ul ì „ì²´ì—ì„œ "í•„ë…" ê²€ìƒ‰ (ìµœí›„ì˜ ìˆ˜ë‹¨)
            if not is_pinned:
                ul_text = ul.get_text(strip=True)
                if "í•„ë…" in ul_text:
                    # ì²« ë²ˆì§¸ liì— "í•„ë…"ì´ ìˆëŠ”ì§€ í™•ì¸
                    first_li = ul.select_one("li:first-child")
                    if first_li and "í•„ë…" in first_li.get_text(strip=True):
                        is_pinned = True
            
            # 4. ë°ì´í„° íŒŒë¼ë¯¸í„° íŒŒì‹±
            raw_params = a.get("data-params", "")
            if not raw_params: continue
            
            # íƒ€ì… ì•ˆì „ì„±ì„ ìœ„í•´ ë¬¸ìì—´ë¡œ ë³€í™˜ (BeautifulSoupì˜ _AttributeValue íƒ€ì… ì²˜ë¦¬)
            raw_params_str: str = str(raw_params)
            if not raw_params_str: continue
            
            # JSON íŒŒì‹± (í™‘ë”°ì˜´í‘œ ì²˜ë¦¬ í¬í•¨)
            try:
                params = json.loads(raw_params_str)
            except:
                params = json.loads(raw_params_str.replace("'", '"'))

            if not (params.get("encMenuSeq") and params.get("encMenuBoardSeq")):
                continue

            # 5. ìµœì¢… ìƒì„¸ URL ìƒì„±
            detail_url = (
                f"{info_url}"
                f"?scrtWrtYn={'true' if params.get('scrtWrtYn') else 'false'}"
                f"&encMenuSeq={params.get('encMenuSeq')}"
                f"&encMenuBoardSeq={params.get('encMenuBoardSeq')}"
            )
            
            # ê¸°ì¡´ì— ê°™ì€ URLì´ ìˆìœ¼ë©´ is_pinned ê°’ ìœ ì§€ (í•„ë…ì´ ìš°ì„ )
            if detail_url in candidates:
                existing_is_pinned = candidates[detail_url].get("is_pinned", False)
                is_pinned = existing_is_pinned or is_pinned
            
            candidates[detail_url] = {"title": final_title.strip(), "is_pinned": is_pinned}
            
        except Exception as e:
            logger.debug(f"Row parsing error: {e}")
            continue
    
    return candidates


# async def _crawl_library_list(category: str, config: dict) -> Dict[str, Dict[str, Any]]:
#     domain = str(config.get("domain", ""))
#     endpoint = str(config.get("url_path", "/Board?n=notice"))
#     full_url = urljoin(domain, endpoint)

#     try:
#         html_text = await fetch_html(full_url)
#         soup = BeautifulSoup(html_text, "html.parser")
#     except Exception as e:
#         logger.error(f"âŒ [{category}] ë„ì„œê´€ ì ‘ì† ì‹¤íŒ¨: {e}")
#         return {}

#     candidates: Dict[str, Dict[str, Any]] = {}
#     items = soup.select("dl.onroad-board dd") or soup.select("a[href*='Board/Detail']")

#     for item in items:
#         try:
#             a_tag = item.find("a") if item.name == "dd" else item
#             if not isinstance(a_tag, Tag): continue

#             link_href = a_tag.get("href")
#             if not isinstance(link_href, str) or "n=notice" not in link_href: continue
            
#             # ë„ì„œê´€ì˜ ê²½ìš° ë³´í†µ 'notice' ì•„ì´ì½˜ì´ë‚˜ í…ìŠ¤íŠ¸ë¡œ êµ¬ë¶„
#             is_pinned = "ê³µì§€" in item.get_text() or "notice" in str(item.get("class", ""))

#             import copy
#             a_clone = copy.copy(a_tag)
#             for tag in a_clone.select("span, i, em"): tag.decompose()
#             title = a_clone.get_text(" ", strip=True)

#             if not title or len(title) < 2: continue

#             full_detail_url = urljoin(domain, link_href)
#             candidates[full_detail_url] = {"title": title, "is_pinned": is_pinned}
#         except: continue

#     return candidates


# async def _crawl_daeple_list(category: str, config: dict) -> Dict[str, Dict[str, Any]]:
#     list_url, info_base_url, _ = get_urls(category)
#     if not list_url: return {}

#     try:
#         html_text = await fetch_html(list_url)
#         soup = BeautifulSoup(html_text, "html.parser")
#     except Exception as e:
#         logger.error(f"âŒ [{category}] ëŒ€í”Œ ì ‘ì† ì‹¤íŒ¨: {e}")
#         return {}

#     candidates: Dict[str, Dict[str, Any]] = {}
#     parsed_list_url = urlparse(list_url)
#     base_query_params = dict(parse_qsl(parsed_list_url.query))
    
#     rows = soup.select(".bbsBoard tbody tr") or soup.select("table tbody tr")

#     for i, row in enumerate(rows):
#         try:
#             # [í•„ë… ê°ì§€] ëŒ€í”Œì€ ë³´í†µ ì²« ë²ˆì§¸ tdì— 'ê³µì§€' ì´ë¯¸ì§€ê°€ ìˆìŒ
#             is_pinned = False
#             first_td = row.select_one("td")
#             if first_td and (first_td.select_one("img") or "ê³µì§€" in first_td.get_text()):
#                 is_pinned = True

#             title_cell = row.select_one(".ellipsis, .subject") or row.find('a').parent
#             a_tag = title_cell.find("a") if title_cell else None
#             if not isinstance(a_tag, Tag): continue

#             title = a_tag.get_text(" ", strip=True)
#             href = str(a_tag.get("href") or a_tag.get("onclick") or "")
#             if len(title) < 2: continue

#             args = re.findall(r"['\"]([^'\"]*)['\"]", href)
#             if len(args) >= 3:
#                 detail_params = {
#                     "bbsId": args[2],
#                     "nttSn": args[0],
#                     **base_query_params
#                 }
#                 full_detail_url = f"{info_base_url}?{urlencode(detail_params)}"
#                 candidates[full_detail_url] = {"title": title, "is_pinned": is_pinned}

#         except Exception as e:
#             logger.debug(f"Row {i} skip: {e}")
#             continue
        
#     return candidates


async def _process_candidates(db: AsyncSession, category: str, candidates_map: Dict[str, Dict[str, Any]]):
    candidate_urls = list(candidates_map.keys())
    if not candidate_urls: return

    try:
        # ê¸°ì¡´ ê³µì§€ì‚¬í•­ ì¡°íšŒ (ë§í¬ì™€ is_pinned ì •ë³´ í¬í•¨)
        stmt = select(Notice).where(and_(Notice.category == category, Notice.link.in_(candidate_urls)))
        result = await db.execute(stmt)
        existing_notices = {notice.link: notice for notice in result.scalars().all()}
        existing_links = set(existing_notices.keys())
    except Exception as e:
        logger.error(f"ğŸ”¥ [{category}] DB ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return

    tasks = []      
    meta_info = []  
    processed_in_this_run = set()
    pinned_count = 0  # í•„ë… ê³µì§€ ì¹´ìš´íŠ¸

    # ê¸°ì¡´ ê³µì§€ì‚¬í•­ì˜ í•„ë… ìƒíƒœ ì—…ë°ì´íŠ¸
    updated_count = 0
    for url, data in candidates_map.items():
        if url in existing_notices:
            existing_notice = existing_notices[url]
            new_is_pinned = data["is_pinned"]
            old_is_pinned = existing_notice.is_pinned
            if old_is_pinned != new_is_pinned:
                existing_notice.is_pinned = new_is_pinned
                updated_count += 1
        if data["is_pinned"]:
            pinned_count += 1

    # ì‹ ê·œ ê³µì§€ì‚¬í•­ ì²˜ë¦¬
    for url, data in candidates_map.items():
        if url in existing_links or url in processed_in_this_run: continue
        processed_in_this_run.add(url)
        is_pinned_flag = data.get("is_pinned", False)
        tasks.append(safe_scrape_with_semaphore(url))
        meta_info.append({"list_title": data.get("title", ""), "is_pinned": bool(is_pinned_flag), "detail_url": url, "category": category})
    
    if updated_count > 0:
        try:
            await db.commit()
            logger.info(f"âœ… [{category}] ê¸°ì¡´ ê³µì§€ {updated_count}ê°œ í•„ë… ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except Exception as e:
            await db.rollback()
            logger.error(f"ğŸ”¥ [{category}] í•„ë… ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    if pinned_count > 0:
        logger.info(f"ğŸ“Œ [{category}] ì´ {pinned_count}ê°œ í•„ë… ê³µì§€ ê°ì§€ë¨")

    # [ì¶”ê°€] í¬ë¡¤ë§ ëª©ë¡ì— ì—†ëŠ” í•„ë… ê³µì§€ ìë™ í•´ì œ (ì‹œê°„ì´ ì§€ë‚˜ ëª©ë¡ì—ì„œ ì‚¬ë¼ì§„ í•„ë… ê³µì§€ ì²˜ë¦¬)
    try:
        crawled_urls = set(candidates_map.keys())
        
        # ë°©ë²• 1: í¬ë¡¤ë§ ëª©ë¡ì— ì—†ëŠ” í•„ë… ê³µì§€ í•´ì œ
        stmt_old_pinned = select(Notice).where(
            Notice.category == category,
            Notice.is_pinned == True,
            Notice.link.not_in(crawled_urls) if crawled_urls else Notice.link != ""
        )
        result_old_pinned = await db.execute(stmt_old_pinned)
        old_pinned_notices = result_old_pinned.scalars().all()
        
        unpinned_count = 0
        for notice in old_pinned_notices:
            notice.is_pinned = False
            unpinned_count += 1
            logger.info(f"ğŸ“Œâ†’ğŸ“„ [{category}] í•„ë… í•´ì œ: {notice.title[:50]}...")
        
        # ë°©ë²• 2: ì•ˆì „ì¥ì¹˜ - 30ì¼ ì´ìƒ ëœ í•„ë… ê³µì§€ ìë™ í•´ì œ
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=30)
        stmt_expired = select(Notice).where(
            Notice.category == category,
            Notice.is_pinned == True,
            Notice.crawled_at < cutoff_date
        )
        result_expired = await db.execute(stmt_expired)
        expired_notices = result_expired.scalars().all()
        
        expired_count = 0
        for notice in expired_notices:
            if notice not in old_pinned_notices:  # ì¤‘ë³µ ë°©ì§€
                notice.is_pinned = False
                expired_count += 1
                logger.info(f"â° [{category}] ì˜¤ë˜ëœ í•„ë… ìë™ í•´ì œ (30ì¼+): {notice.title[:50]}...")
        
        total_unpinned = unpinned_count + expired_count
        if total_unpinned > 0:
            await db.commit()
            logger.info(f"âœ… [{category}] {total_unpinned}ê°œ í•„ë… ê³µì§€ ìë™ í•´ì œ ì™„ë£Œ (ëª©ë¡ ë°–: {unpinned_count}, ê¸°í•œ ë§Œë£Œ: {expired_count})")
    except Exception as e:
        logger.error(f"âš ï¸ [{category}] í•„ë… ìë™ í•´ì œ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ì€ ê³„ì† ì§„í–‰

    if not tasks: return

    logger.info(f"ğŸš€ [{category}] {len(tasks)}ê°œ ì‹ ê·œ ìƒì„¸ ìˆ˜ì§‘ ì‹œì‘")
    results = await asyncio.gather(*tasks, return_exceptions=True)
    new_notices_buffer = [] 
    
    for i, result in enumerate(results):
        if isinstance(result, Exception) or not result: 
            continue
        
        if i >= len(meta_info):
            logger.error(f"ğŸ”¥ [{category}] ì¸ë±ìŠ¤ ë¶ˆì¼ì¹˜: result ì¸ë±ìŠ¤ {i} >= meta_info ê¸¸ì´ {len(meta_info)}")
            continue
            
        scraped_data = cast(Dict[str, Any], result)
        meta = meta_info[i]
        
        final_title = str(scraped_data.get("title") or meta["list_title"])
        content_lines = scraped_data.get("texts", [])
        final_content = "\n\n".join(content_lines) if isinstance(content_lines, list) else ""
        is_pinned_value = meta.get("is_pinned", False)
        
        new_notice = Notice(
            title=final_title,
            link=meta["detail_url"],
            date=scraped_data.get("date"),
            content=final_content,
            images=scraped_data.get("images", []),
            files=scraped_data.get("files", []),
            category=meta["category"],
            univ_views=scraped_data.get("univ_views", 0),
            app_views=0,
            is_pinned=is_pinned_value # [ì¶”ê°€] í•„ë… ì—¬ë¶€ ì €ì¥
        )
        
        if is_pinned_value:
            logger.info(f"ğŸ“Œ [{category}] í•„ë… ê³µì§€ ì €ì¥: {final_title[:60]}...")
        
        db.add(new_notice)
        new_notices_buffer.append(new_notice)

    if new_notices_buffer:
        try:
            await db.commit()
            logger.info(f"âœ… [{category}] {len(new_notices_buffer)}ê°œ ì €ì¥ ì™„ë£Œ")
            
            if category in NOTIFICATION_TARGET_CATEGORIES:
                await send_keyword_notifications(db, new_notices_buffer)
        except Exception as e:
            await db.rollback()
            logger.error(f"ğŸ”¥ DB ì»¤ë°‹ ì‹¤íŒ¨: {e}")


async def safe_scrape_with_semaphore(url: str):
    async with SCRAPE_SEMAPHORE:
        await asyncio.sleep(0.5) 
        return await scrape_notice_content(url)


async def search_notices(
    db: AsyncSession, 
    category: str, 
    query: Optional[str] = None, 
    skip: int = 0, 
    limit: int = 20, 
    sort_by: str = "date"
):
    """ê³µì§€ì‚¬í•­ ê²€ìƒ‰ ë° ì¡°íšŒ (APIìš©) - í•„ë… ìš°ì„  ì •ë ¬ ì ìš©"""
    stmt = select(Notice)
    if category != "all":
        stmt = stmt.where(Notice.category == category)
    if query:
        stmt = stmt.where(Notice.title.like(f"%{query}%"))
    
    # [ìˆ˜ì •] 1ìˆœìœ„: í•„ë…(is_pinned), 2ìˆœìœ„: ë‚ ì§œ(date), 3ìˆœìœ„: ID
    if sort_by == "views":
        stmt = stmt.order_by(Notice.is_pinned.desc(), Notice.univ_views.desc())
    else:
        stmt = stmt.order_by(Notice.is_pinned.desc(), Notice.date.desc().nulls_last(), Notice.id.desc())
        
    stmt = stmt.offset(skip).limit(limit)
    result = await db.execute(stmt)
    return result.scalars().all()

# ... (ê¸°ì¡´ get_or_create_summary í•¨ìˆ˜ ìœ ì§€)

async def get_or_create_summary(db: AsyncSession, notice_id: int) -> str:
    stmt = select(Notice).where(Notice.id == notice_id)
    result = await db.execute(stmt)
    notice = result.scalars().first()
    
    if not notice:
        raise ValueError("Notice not found")
        
    if notice.summary:
        return notice.summary

    content_to_use = notice.content or ""
    image_list = []
    
    if notice.images:
        try: image_list = json.loads(str(notice.images))
        except: pass

    if len(content_to_use) < 5:
        logger.info(f"ğŸ” [Auto-Rescrape] ID:{notice_id} ë³¸ë¬¸ ë³´ê°• ì‹œë„")
        scraped_data = await scrape_notice_content(notice.link)
        if scraped_data:
            content_to_use = "\n\n".join(scraped_data.get("texts", []))
            image_list = scraped_data.get("images", [])
            notice.content = content_to_use
            try:
                notice.images = image_list 
            except: pass
            
    summary = await generate_summary(content_to_use, image_list)
    notice.summary = summary
    await db.commit()
    return summary