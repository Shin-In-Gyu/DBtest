import json
import html as html_lib
import asyncio
from bs4 import BeautifulSoup
from sqlalchemy.orm import Session
from sqlalchemy import or_

# [ì„¤ì • ë° ìœ í‹¸ë¦¬í‹°] í”„ë¡œì íŠ¸ ë‚´ ë‹¤ë¥¸ íŒŒì¼ë“¤ì—ì„œ í•„ìš”í•œ ê¸°ëŠ¥ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
from app.core.config import get_urls 
from app.core.http import fetch_html
from app.database.models import Notice

# [ìŠ¤í¬ë˜í¼] ìƒì„¸ í˜ì´ì§€ì˜ ë‚´ë¶€(ë³¸ë¬¸, íŒŒì¼, ì´ë¯¸ì§€)ë¥¼ ê¸ì–´ì˜¤ëŠ” í•µì‹¬ í•¨ìˆ˜ì…ë‹ˆë‹¤.
from app.services.scraper import scrape_notice_content

async def crawl_and_sync_notices(db: Session, category: str = "univ"):
    """
    ì§€ì •ëœ ì¹´í…Œê³ ë¦¬(ì˜ˆ: 'univ', 'job' ë“±)ì˜ ê³µì§€ì‚¬í•­ì„ í¬ë¡¤ë§í•˜ì—¬ DBì™€ ë™ê¸°í™”í•©ë‹ˆë‹¤.
    
    1. ëª©ë¡ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ê²Œì‹œê¸€ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    2. ê° ê²Œì‹œê¸€ì˜ ìƒì„¸ í˜ì´ì§€ ë§í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    3. DBì— ì—†ëŠ” ìƒˆë¡œìš´ ê¸€ì´ë¼ë©´ ìƒì„¸ í˜ì´ì§€ ë‚´ìš©ì„ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.
    4. ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€(JSON), íŒŒì¼(JSON)ë¡œ ì •ë¦¬í•˜ì—¬ DBì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”„ [{category}] ë™ê¸°í™” ì‘ì—… ì‹œì‘...")
    
    # 1. ì„¤ì • íŒŒì¼(config.py)ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ URL ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    list_url, info_url, default_seq = get_urls(category)
    
    # 2. ëª©ë¡ í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸° (ë„¤íŠ¸ì›Œí¬ ìš”ì²­)
    try:
        # fetch_html: ë¹„ë™ê¸° HTTP ìš”ì²­ì„ ë³´ë‚´ HTML í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤.
        html_text = await fetch_html(list_url, params={"searchMenuSeq": default_seq})
        soup = BeautifulSoup(html_text, "html.parser") # HTML íŒŒì‹± ì¤€ë¹„
    except Exception as e:
        print(f"âŒ [{category}] ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return

    new_count = 0 # ìƒˆë¡œ ì €ì¥ëœ ê¸€ì˜ ê°œìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•œ ë³€ìˆ˜
    
    # [ë°©ì–´ë§‰ 1] ì´ë²ˆ ì‹¤í–‰ ë£¨í”„ì—ì„œ ì²˜ë¦¬í•œ URLì„ ê¸°ë¡í•˜ëŠ” ì§‘í•© (ìƒë‹¨ ê³ ì • ê³µì§€ ì¤‘ë³µ ë°©ì§€ìš©)
    processed_links = set() 
    
    # 3. HTMLì—ì„œ ê³µì§€ì‚¬í•­ ë¦¬ìŠ¤íŠ¸(<a> íƒœê·¸) ì¶”ì¶œí•˜ì—¬ ë°˜ë³µë¬¸ ì‹¤í–‰
    # select("a.detailLink[data-params]") : classê°€ detailLinkì´ê³  data-params ì†ì„±ì´ ìˆëŠ” <a> íƒœê·¸ë§Œ ì°¾ìŒ
    for a in soup.select("a.detailLink[data-params]"):
        try:
            # -------------------------------------------------------
            # [Step 1] ëª©ë¡ì—ì„œ ê¸°ë³¸ ì •ë³´(ì œëª©, íŒŒë¼ë¯¸í„°) ì¶”ì¶œ
            # -------------------------------------------------------
            list_title = a.get_text(" ", strip=True) or a.get("title", "").strip()
            
            # data-params ì†ì„±ì— ìˆëŠ” ì•”í˜¸í™”ëœ íŒŒë¼ë¯¸í„° ë¬¸ìì—´ì„ ê°€ì ¸ì˜´ (HTML íŠ¹ìˆ˜ë¬¸ì í•´ë… í¬í•¨)
            raw = html_lib.unescape(a.get("data-params", "")).strip()

            # JSON í˜•íƒœì˜ ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            try:
                params = json.loads(raw)
            except Exception:
                # ê°€ë” JSON í‘œì¤€ì¸ í°ë”°ì˜´í‘œ(") ëŒ€ì‹  ì‘ì€ë”°ì˜´í‘œ(')ë¥¼ ì“°ëŠ” ê²½ìš°ê°€ ìˆì–´ ì˜ˆì™¸ ì²˜ë¦¬
                try:
                    params = json.loads(raw.replace("'", '"'))
                except Exception:
                    continue # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ ê¸€ ê±´ë„ˆëœ€

            # ë§í¬ ìƒì„±ì— í•„ìš”í•œ í•µì‹¬ ID ê°’ ì¶”ì¶œ
            enc_menu_seq = params.get("encMenuSeq")
            enc_menu_board_seq = params.get("encMenuBoardSeq")
            scrt_wrt_yn = params.get("scrtWrtYn", False) # ë¹„ë°€ê¸€ ì—¬ë¶€

            # í•„ìˆ˜ íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
            if not (enc_menu_seq and enc_menu_board_seq):
                continue

            # -------------------------------------------------------
            # [Step 2] ìƒì„¸ í˜ì´ì§€ ì ‘ì†ì„ ìœ„í•œ ìµœì¢… URL ì¡°ë¦½
            # -------------------------------------------------------
            detail_url = (
                f"{info_url}"
                f"?scrtWrtYn={'true' if scrt_wrt_yn else 'false'}"
                f"&encMenuSeq={enc_menu_seq}"
                f"&encMenuBoardSeq={enc_menu_board_seq}"
            )

            # [ì¤‘ë³µ ê²€ì‚¬ 1] ë°©ê¸ˆ ì²˜ë¦¬í•œ ë§í¬ë¼ë©´ ê±´ë„ˆëœ€ (ìƒë‹¨ ê³ ì • ê³µì§€ê°€ ëª©ë¡ì— ì¤‘ë³µ ë…¸ì¶œë˜ëŠ” ê²½ìš° ë°©ì§€)
            if detail_url in processed_links:
                continue
            processed_links.add(detail_url)

            # [ì¤‘ë³µ ê²€ì‚¬ 2] ì´ë¯¸ DBì— ì €ì¥ëœ ë§í¬ë¼ë©´ ê±´ë„ˆëœ€ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„!)
            # filter(Notice.link == detail_url) : ë§í¬ê°€ ì¼ì¹˜í•˜ëŠ” ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
            if db.query(Notice).filter(Notice.link == detail_url).first():
                continue

            # -------------------------------------------------------
            # [Step 3] ìƒì„¸ í˜ì´ì§€ ë‚´ìš© ìŠ¤í¬ë˜í•‘ (scraper.py í˜¸ì¶œ)
            # -------------------------------------------------------
            try:
                # ì—¬ê¸°ì„œ ì‹¤ì œë¡œ ìƒì„¸ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ì œëª©, ë³¸ë¬¸, ì´ë¯¸ì§€, íŒŒì¼ ì •ë³´ë¥¼ ê°€ì ¸ì˜´
                # scraped_data = {"title": "...", "texts": [...], "images": [...], "files": [...]}
                scraped_data = await scrape_notice_content(detail_url)
            except Exception as e:
                print(f"   âš ï¸ ìƒì„¸ í¬ë¡¤ë§ ì‹¤íŒ¨ ({list_title}): {e}")
                continue

            # -------------------------------------------------------
            # [Step 4] ë°ì´í„° ê°€ê³µ ë° DB ê°ì²´ ìƒì„±
            # -------------------------------------------------------
            
            # (1) ì œëª© ê²°ì •: ìƒì„¸ í˜ì´ì§€ ì•ˆì˜ ì œëª©ì´ ë” ì •í™•í•˜ë¯€ë¡œ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ëª©ë¡ ì œëª© ì‚¬ìš©
            final_title = scraped_data["title"] if scraped_data["title"] else list_title
            
            # (2) ë³¸ë¬¸ ë³‘í•©: ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ë‰œ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
            content_body = "\n\n".join(scraped_data["texts"])

            # (3) [ì¤‘ìš”] ì´ë¯¸ì§€ì™€ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
            # DBì—ëŠ” ë¦¬ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì €ì¥í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ í…ìŠ¤íŠ¸(JSON) í˜•íƒœë¡œ ë³€í™˜í•´ì„œ ì €ì¥í•´ì•¼ í•¨
            # ensure_ascii=False : í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê³  ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ì €ì¥ë¨
            images_json = json.dumps(scraped_data["images"], ensure_ascii=False)
            files_json = json.dumps(scraped_data["files"], ensure_ascii=False)

            # (4) Notice ëª¨ë¸ ê°ì²´ ìƒì„± (DB í…Œì´ë¸”ì˜ í•œ í–‰ì„ ë§Œë“¦)
            new_notice = Notice(
                title=final_title,
                link=detail_url,
                content=content_body, # ìˆœìˆ˜ ë³¸ë¬¸ í…ìŠ¤íŠ¸
                images=images_json,   # ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸ (JSON String)
                files=files_json,     # íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (JSON String)
                category=category,    # í˜„ì¬ ì¹´í…Œê³ ë¦¬ (univ, job ë“±)
            )
            
            # DB ì„¸ì…˜ì— ì¶”ê°€ (ì•„ì§ ì»¤ë°‹ì€ ì•ˆ í•¨)
            db.add(new_notice)
            new_count += 1
            
            # [ë§¤ë„ˆ] ì„œë²„ì— ë„ˆë¬´ ë¹ ë¥¸ ìš”ì²­ì„ ë³´ë‚´ì§€ ì•Šë„ë¡ 0.1ì´ˆ ëŒ€ê¸°
            await asyncio.sleep(0.1) 

        except Exception as e:
            print(f"âš ï¸ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
            continue

    # -------------------------------------------------------
    # [Step 5] ìµœì¢… DB ì €ì¥ (Commit)
    # -------------------------------------------------------
    try:
        db.commit() # ëª¨ì•„ë‘” ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì‹¤ì œ DBì— ë°˜ì˜
        if new_count > 0:
            print(f"âœ… [{category}] {new_count}ê°œ ì‹ ê·œ ê³µì§€ ì €ì¥ ì™„ë£Œ!")
        else:
            print(f"ğŸ’¤ [{category}] ìƒˆë¡œìš´ ê³µì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        db.rollback() # ì—ëŸ¬ ë°œìƒ ì‹œ ë³€ê²½ ì‚¬í•­ì„ ëª¨ë‘ ì·¨ì†Œ (ë°ì´í„° ë¬´ê²°ì„± ë³´í˜¸)
        print(f"ğŸ”¥ DB ì €ì¥ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ (ë¡¤ë°±ë¨): {e}")


# [ì¡°íšŒ í•¨ìˆ˜] APIì—ì„œ í˜¸ì¶œí•˜ì—¬ DBì— ì €ì¥ëœ ê³µì§€ì‚¬í•­ì„ ê²€ìƒ‰/ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
def search_notices_from_db(db: Session, category: str, query: str = None, skip: int = 0, limit: int = 20):
    """
    DBì—ì„œ ê³µì§€ì‚¬í•­ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    - category: íŠ¹ì • ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ('all'ì´ë©´ ì „ì²´)
    - query: ì œëª© ë˜ëŠ” ë³¸ë¬¸ì— í¬í•¨ëœ ê²€ìƒ‰ì–´
    - skip, limit: í˜ì´ì§• ì²˜ë¦¬ (skipë§Œí¼ ê±´ë„ˆë›°ê³  limitë§Œí¼ ê°€ì ¸ì˜´)
    """
    sql = db.query(Notice)
    
    # ì¹´í…Œê³ ë¦¬ í•„í„°ë§
    if category != "all":
        sql = sql.filter(Notice.category == category)
    
    # ê²€ìƒ‰ì–´ í•„í„°ë§ (ì œëª© OR ë³¸ë¬¸)
    if query:
        search_filter = f"%{query}%"
        sql = sql.filter(
            or_(Notice.title.like(search_filter), Notice.content.like(search_filter))
        )
    
    # ìµœì‹ ìˆœ(id ë‚´ë¦¼ì°¨ìˆœ) ì •ë ¬ í›„ ê²°ê³¼ ë°˜í™˜
    return sql.order_by(Notice.id.desc()).offset(skip).limit(limit).all()