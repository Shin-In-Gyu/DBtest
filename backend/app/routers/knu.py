# app/routers/knu.py
import json
import traceback
from typing import List, Optional, Set

from fastapi import APIRouter, Depends, Query, HTTPException
from sqlalchemy.orm import Session

from app.database.database import get_db
from app.database.models import Notice, Device, Scrap
from app.schemas import (
    NoticeListResponse, 
    NoticeDetailResponse, 
    DeviceRegisterRequest, 
    ScrapRequest
)
from app.services import knu_notice_service
from app.services.scraper import scrape_notice_content
from app.services.ai_service import generate_summary
from app.core.logger import get_logger

router = APIRouter()
logger = get_logger()

# --------------------------------------------------------------------------
# 1. ê³µì§€ì‚¬í•­ ëª©ë¡ ì¡°íšŒ
# --------------------------------------------------------------------------
@router.get("/notices", response_model=List[NoticeListResponse])
async def read_notices(
    category: str = "all",
    q: Optional[str] = Query(None, description="ê²€ìƒ‰ì–´"),
    page: int = 1,
    sort_by: str = "date",
    token: Optional[str] = Query(None, description="ìŠ¤í¬ë© í™•ì¸ìš© í† í°"),
    db: Session = Depends(get_db)
):
    limit = 20
    skip = (page - 1) * limit
    
    # DB ì¡°íšŒ
    results = knu_notice_service.search_notices(
        db, category, query=q, skip=skip, limit=limit, sort_by=sort_by
    )

    # ìŠ¤í¬ë© ì—¬ë¶€ ë§ˆí‚¹ (Setì„ ì‚¬ìš©í•˜ì—¬ O(1) ì¡°íšŒ ì†ë„ í™•ë³´)
    if token:
        device = db.query(Device).filter(Device.token == token).first()
        if device:
            my_scrap_ids: Set[int] = {
                s.notice_id for s in db.query(Scrap.notice_id).filter(Scrap.device_id == device.id).all()
            }
            for notice in results:
                if notice.id in my_scrap_ids:
                    notice.is_scraped = True

    return results

# --------------------------------------------------------------------------
# 2. ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ (ë³¸ë¬¸ ì—…ë°ì´íŠ¸ ë° ì •ë³´ ë°˜í™˜)
# --------------------------------------------------------------------------
@router.get("/notice/detail", response_model=NoticeDetailResponse)
async def get_notice_detail(
    url: str, 
    notice_id: Optional[int] = None, 
    token: Optional[str] = Query(None),
    db: Session = Depends(get_db)
):
    notice_in_db = None
    is_scraped = False

    # A. DB ì¡°íšŒ ë° ì¡°íšŒìˆ˜ ì¦ê°€
    if notice_id:
        notice_in_db = db.query(Notice).filter(Notice.id == notice_id).first()
        if notice_in_db:
            try:
                # ì¡°íšŒìˆ˜ 1 ì¦ê°€ (Dirty Read ë°©ì§€ ìœ„í•´ ë³„ë„ ì¿¼ë¦¬ ê¶Œì¥ë˜ë‚˜ ì—¬ê¸°ì„  ê°„ë‹¨íˆ ì²˜ë¦¬)
                notice_in_db.app_views += 1
                db.commit()
            except:
                db.rollback()

            # ìŠ¤í¬ë© ì—¬ë¶€ í™•ì¸
            if token:
                device = db.query(Device).filter(Device.token == token).first()
                if device and device.id:
                    exists = db.query(Scrap).filter(
                        Scrap.device_id == device.id, 
                        Scrap.notice_id == notice_id
                    ).first()
                    is_scraped = bool(exists)

    # B. ì‹¤ì‹œê°„ ë‚´ìš© í¬ë¡¤ë§
    scraped_data = await scrape_notice_content(url)
    if not scraped_data:
        raise HTTPException(status_code=404, detail="ì›ë¬¸ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # C. DBì— ë³¸ë¬¸ í…ìŠ¤íŠ¸ ë°±ì—… (AI ìš”ì•½ì„ ìœ„í•¨)
    if notice_in_db:
        new_full_content = "\n\n".join(scraped_data["texts"])
        # ë‚´ìš©ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ë¹„ì–´ìˆì—ˆë‹¤ë©´ ì—…ë°ì´íŠ¸
        if notice_in_db.content != new_full_content:
            notice_in_db.content = new_full_content
            try:
                db.commit()
            except:
                db.rollback()

    return {
        "id": notice_id if notice_id else 0,
        "title": scraped_data["title"],
        "link": url,
        "date": scraped_data["date"],
        "category": notice_in_db.category if notice_in_db else "unknown",
        "author": notice_in_db.author if notice_in_db else None,
        "content": "\n\n".join(scraped_data["texts"]),
        "images": scraped_data["images"],
        "files": scraped_data["files"],
        "univ_views": scraped_data["univ_views"],
        "app_views": notice_in_db.app_views if notice_in_db else 0,
        "crawled_at": notice_in_db.crawled_at if notice_in_db else None,
        "is_scraped": is_scraped,
        "summary": notice_in_db.summary if notice_in_db else None
    }

# --------------------------------------------------------------------------
# 3. AI ìš”ì•½ ìƒì„± (On-Demand)
# --------------------------------------------------------------------------
@router.post("/notice/{notice_id}/summary")
async def create_notice_summary(notice_id: int, db: Session = Depends(get_db)):
    try:
        notice = db.query(Notice).filter(Notice.id == notice_id).first()
        if not notice:
            raise HTTPException(status_code=404, detail="ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì´ë¯¸ ìš”ì•½ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜ (ìºì‹± íš¨ê³¼)
        if notice.summary:
            return {"summary": notice.summary}

        # ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ íŒŒì‹± (JSON ì—ëŸ¬ ë°©ì§€)
        image_list = []
        if notice.images:
            try:
                raw_images = str(notice.images).strip()
                if raw_images and raw_images.lower() != "none":
                    image_list = json.loads(raw_images)
            except json.JSONDecodeError:
                image_list = []

        # ìš”ì•½ ëŒ€ìƒ í…ìŠ¤íŠ¸ ì¤€ë¹„
        content_to_use = notice.content or ""

        # ë‚´ìš© í™•ì¸
        if len(content_to_use) < 10 and not image_list:
             raise HTTPException(status_code=400, detail="ìš”ì•½í•  ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

        # AI í˜¸ì¶œ
        logger.info(f"ğŸ¤– [Gemini] ìš”ì•½ ìš”ì²­: ID {notice_id}")
        summary_text = await generate_summary(content_to_use, image_list)

        if not summary_text:
             raise HTTPException(status_code=500, detail="AI ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # ê²°ê³¼ ì €ì¥
        notice.summary = summary_text
        db.commit()
        
        return {"summary": summary_text}

    except HTTPException as http_ex:
        raise http_ex
    except Exception as e:
        db.rollback()
        logger.error(f"ğŸ”¥ [Summary Error] {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ")

# --------------------------------------------------------------------------
# 4. ê¸°ê¸° ë“±ë¡ ë° ìŠ¤í¬ë© API
# --------------------------------------------------------------------------
@router.post("/device/register")
async def register_device(request: DeviceRegisterRequest, db: Session = Depends(get_db)):
    existing = db.query(Device).filter(Device.token == request.token).first()
    
    if existing:
        existing.keywords = request.keywords
        logger.info(f"ğŸ”„ ê¸°ê¸° ê°±ì‹ : {request.token[:8]}...")
    else:
        new_device = Device(token=request.token, keywords=request.keywords)
        db.add(new_device)
        logger.info(f"âœ¨ ê¸°ê¸° ë“±ë¡: {request.token[:8]}...")
    
    try:
        db.commit()
        return {"message": "success", "keywords": request.keywords}
    except:
        db.rollback()
        raise HTTPException(status_code=500, detail="ê¸°ê¸° ë“±ë¡ ì‹¤íŒ¨")

@router.post("/scrap/{notice_id}")
async def toggle_scrap(notice_id: int, request: ScrapRequest, db: Session = Depends(get_db)):
    device = db.query(Device).filter(Device.token == request.token).first()
    if not device:
        raise HTTPException(status_code=404, detail="ê¸°ê¸° ë“±ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    notice = db.query(Notice).filter(Notice.id == notice_id).first()
    if not notice:
        raise HTTPException(status_code=404, detail="ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

    existing_scrap = db.query(Scrap).filter(
        Scrap.device_id == device.id, 
        Scrap.notice_id == notice_id
    ).first()

    try:
        if existing_scrap:
            db.delete(existing_scrap)
            db.commit()
            return {"status": "removed", "message": "ìŠ¤í¬ë© ì·¨ì†Œë¨"}
        else:
            new_scrap = Scrap(device_id=device.id, notice_id=notice_id)
            db.add(new_scrap)
            db.commit()
            return {"status": "added", "message": "ìŠ¤í¬ë© ì €ì¥ë¨"}
    except Exception as e:
        db.rollback()
        logger.error(f"ìŠ¤í¬ë© ì—ëŸ¬: {e}")
        raise HTTPException(status_code=500, detail="DB Error")

@router.get("/scraps", response_model=List[NoticeListResponse])
async def get_my_scraps(token: str, db: Session = Depends(get_db)):
    device = db.query(Device).filter(Device.token == token).first()
    if not device:
        return []

    scraped_notices = (
        db.query(Notice)
        .join(Scrap, Notice.id == Scrap.notice_id)
        .filter(Scrap.device_id == device.id)
        .order_by(Scrap.created_at.desc())
        .all()
    )

    for notice in scraped_notices:
        notice.is_scraped = True
        
    return scraped_notices