# app/routers/knu.py
import json
import traceback
from typing import List, Optional, Set

from fastapi import APIRouter, Depends, Query, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, delete, or_, desc

from app.database.database import get_db
from app.database.models import Notice, Device, Scrap
from app.schemas import (
    NoticeListResponse, 
    NoticeDetailResponse, 
    DeviceRegisterRequest, 
    ScrapRequest
)
from app.services import knu_notice_service
from app.services.scraper import scrape_notice_content
from app.services.ai_service import generate_summary
from app.core.logger import get_logger

router = APIRouter()
logger = get_logger()

# --------------------------------------------------------------------------
# 1. ê³µì§€ì‚¬í•­ ëª©ë¡ ì¡°íšŒ (Async Refactored)
# --------------------------------------------------------------------------
@router.get("/notices", response_model=List[NoticeListResponse])
async def read_notices(
    category: str = "all",
    q: Optional[str] = Query(None, description="ê²€ìƒ‰ì–´"),
    page: int = 1,
    sort_by: str = "date",
    token: Optional[str] = Query(None, description="ìŠ¤í¬ë© í™•ì¸ìš© í† í°"),
    db: AsyncSession = Depends(get_db)  # Session -> AsyncSession ë³€ê²½
):
    limit = 20
    skip = (page - 1) * limit
    
    # [Fix] ì„œë¹„ìŠ¤ í•¨ìˆ˜ í˜¸ì¶œ ì‹œ await ì¶”ê°€ (knu_notice_serviceê°€ async í•¨ìˆ˜ì„)
    results = await knu_notice_service.search_notices(
        db, category, query=q, skip=skip, limit=limit, sort_by=sort_by
    )

    # ìŠ¤í¬ë© ì—¬ë¶€ ë§ˆí‚¹ (Async ì¿¼ë¦¬ë¡œ ë³€ê²½)
    if token:
        # ê¸°ê¸° ì¡°íšŒ
        stmt_device = select(Device).filter(Device.token == token)
        res_device = await db.execute(stmt_device)
        device = res_device.scalars().first()
        
        if device:
            # ë‚´ ìŠ¤í¬ë© ëª©ë¡ ì¡°íšŒ
            stmt_scrap = select(Scrap.notice_id).filter(Scrap.device_id == device.id)
            res_scrap = await db.execute(stmt_scrap)
            my_scrap_ids = set(res_scrap.scalars().all())
            
            for notice in results:
                if notice.id in my_scrap_ids:
                    notice.is_scraped = True

    return results

# --------------------------------------------------------------------------
# 2. ê³µì§€ì‚¬í•­ ìƒì„¸ ì¡°íšŒ (Async Refactored)
# --------------------------------------------------------------------------
@router.get("/notice/detail", response_model=NoticeDetailResponse)
async def get_notice_detail(
    url: str, 
    notice_id: Optional[int] = None, 
    token: Optional[str] = Query(None),
    db: AsyncSession = Depends(get_db)
):
    notice_in_db = None
    is_scraped = False

    # A. DB ì¡°íšŒ ë° ì¡°íšŒìˆ˜ ì¦ê°€
    if notice_id:
        stmt = select(Notice).filter(Notice.id == notice_id)
        result = await db.execute(stmt)
        notice_in_db = result.scalars().first()

        if notice_in_db:
            try:
                # ì¡°íšŒìˆ˜ 1 ì¦ê°€
                notice_in_db.app_views += 1
                await db.commit() # [Fix] await ì¶”ê°€
            except Exception:
                await db.rollback() # [Fix] await ì¶”ê°€

            # ìŠ¤í¬ë© ì—¬ë¶€ í™•ì¸
            if token:
                stmt_device = select(Device).filter(Device.token == token)
                res_device = await db.execute(stmt_device)
                device = res_device.scalars().first()
                
                if device:
                    stmt_check = select(Scrap).filter(
                        Scrap.device_id == device.id, 
                        Scrap.notice_id == notice_id
                    )
                    res_check = await db.execute(stmt_check)
                    is_scraped = bool(res_check.scalars().first())

    # B. ì‹¤ì‹œê°„ ë‚´ìš© í¬ë¡¤ë§ (ì´ë¯¸ Async í•¨ìˆ˜ì„)
    scraped_data = await scrape_notice_content(url)
    if not scraped_data:
        raise HTTPException(status_code=404, detail="ì›ë¬¸ í˜ì´ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # C. DB ì—…ë°ì´íŠ¸ (Async Commit)
    if notice_in_db:
        new_full_content = "\n\n".join(scraped_data["texts"])
        if notice_in_db.content != new_full_content:
            notice_in_db.content = new_full_content
            try:
                await db.commit()
            except Exception:
                await db.rollback()

    return {
        "id": notice_id if notice_id else 0,
        "title": scraped_data["title"],
        "link": url,
        "date": scraped_data["date"],
        "category": notice_in_db.category if notice_in_db else "unknown",
        "author": notice_in_db.author if notice_in_db else None,
        "content": "\n\n".join(scraped_data["texts"]),
        "images": scraped_data["images"],
        "files": scraped_data["files"],
        "univ_views": scraped_data["univ_views"],
        "app_views": notice_in_db.app_views if notice_in_db else 0,
        "crawled_at": notice_in_db.crawled_at if notice_in_db else None,
        "is_scraped": is_scraped,
        "summary": notice_in_db.summary if notice_in_db else None
    }

# --------------------------------------------------------------------------
# 3. AI ìš”ì•½ ìƒì„± (Async Refactored) - ì—ëŸ¬ ë°œìƒí•˜ë˜ ë¶€ë¶„
# --------------------------------------------------------------------------
@router.post("/notice/{notice_id}/summary")
async def create_notice_summary(notice_id: int, db: AsyncSession = Depends(get_db)):
    try:
        # 1. ê³µì§€ì‚¬í•­ ì¡°íšŒ
        stmt = select(Notice).filter(Notice.id == notice_id)
        result = await db.execute(stmt)
        notice = result.scalars().first()

        if not notice:
            raise HTTPException(status_code=404, detail="ê³µì§€ì‚¬í•­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # 2. ì´ë¯¸ ìš”ì•½ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°˜í™˜
        if notice.summary:
            return {"summary": notice.summary}

        # -----------------------------------------------------------
        # [í•µì‹¬ ìˆ˜ì •] ë³¸ë¬¸ì´ ë¹„ì–´ìˆìœ¼ë©´(50ì ë¯¸ë§Œ) ì¦‰ì‹œ ìŠ¤í¬ë˜í•‘ ì‹œë„
        # -----------------------------------------------------------
        content_to_use = notice.content or ""
        image_list = []

        # DBì— ì €ì¥ëœ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë¡œë“œ
        if notice.images:
            try:
                raw_images = str(notice.images).strip()
                if raw_images and raw_images.lower() != "none":
                    image_list = json.loads(raw_images)
            except:
                image_list = []

        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë¬¸ ë§í¬ì—ì„œ ë‹¤ì‹œ ê¸ì–´ì˜´
        if len(content_to_use) < 50:
            logger.info(f"ğŸ” [Gemini] ë³¸ë¬¸ ëˆ„ë½ë¨. ì‹¤ì‹œê°„ ìŠ¤í¬ë˜í•‘ ì‹œë„: {notice.link}")
            scraped_data = await scrape_notice_content(notice.link)
            
            if scraped_data:
                # ìŠ¤í¬ë˜í•‘ ì„±ê³µ ì‹œ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                content_to_use = "\n\n".join(scraped_data["texts"])
                image_list = scraped_data["images"]
                
                # DBì—ë„ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ë²ˆ ìš”ì²­ì„ ìœ„í•´)
                notice.content = content_to_use
                notice.images = json.dumps(image_list, ensure_ascii=False)
                # (ì£¼ì˜: ì—¬ê¸°ì„œ commitì€ í•˜ì§€ ì•Šê³ , ë§ˆì§€ë§‰ì— ìš”ì•½ ì €ì¥í•  ë•Œ í•œ ë²ˆì— í•©ë‹ˆë‹¤)

        # -----------------------------------------------------------

        # 3. ê·¸ë˜ë„ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ì²˜ë¦¬
        if len(content_to_use) < 10 and not image_list:
             # AIì—ê²Œ ë³´ë‚¼ ë‚´ìš©ì´ ì •ë§ ì—†ëŠ” ê²½ìš°
             return {"summary": "ë³¸ë¬¸ì´ ì´ë¯¸ì§€ë‚˜ ì²¨ë¶€íŒŒì¼ë¡œë§Œ êµ¬ì„±ë˜ì–´ ìš”ì•½í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."}

        # 4. AI í˜¸ì¶œ
        logger.info(f"ğŸ¤– [Gemini] ìš”ì•½ ìš”ì²­: ID {notice_id} (Text Len: {len(content_to_use)})")
        summary_text = await generate_summary(content_to_use, image_list)

        if not summary_text:
             raise HTTPException(status_code=500, detail="AI ì‘ë‹µ ì‹¤íŒ¨")

        # 5. ê²°ê³¼ ì €ì¥ ë° ì»¤ë°‹
        notice.summary = summary_text
        await db.commit()
        
        return {"summary": summary_text}

    except HTTPException as http_ex:
        raise http_ex
    except Exception as e:
        await db.rollback()
        logger.error(f"ğŸ”¥ [Summary Error] {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ")
# --------------------------------------------------------------------------
# 4. ê¸°ê¸° ë“±ë¡ ë° ìŠ¤í¬ë© API (Async Refactored)
# --------------------------------------------------------------------------
@router.post("/device/register")
async def register_device(request: DeviceRegisterRequest, db: AsyncSession = Depends(get_db)):
    stmt = select(Device).filter(Device.token == request.token)
    result = await db.execute(stmt)
    existing = result.scalars().first()
    
    if existing:
        existing.keywords = request.keywords
        logger.info(f"ğŸ”„ ê¸°ê¸° ê°±ì‹ : {request.token[:8]}...")
    else:
        new_device = Device(token=request.token, keywords=request.keywords)
        db.add(new_device)
        logger.info(f"âœ¨ ê¸°ê¸° ë“±ë¡: {request.token[:8]}...")
    
    try:
        await db.commit()
        return {"message": "success", "keywords": request.keywords}
    except:
        await db.rollback()
        raise HTTPException(status_code=500, detail="ê¸°ê¸° ë“±ë¡ ì‹¤íŒ¨")

@router.post("/scrap/{notice_id}")
async def toggle_scrap(notice_id: int, request: ScrapRequest, db: AsyncSession = Depends(get_db)):
    # ê¸°ê¸° í™•ì¸
    res_device = await db.execute(select(Device).filter(Device.token == request.token))
    device = res_device.scalars().first()
    if not device:
        raise HTTPException(status_code=404, detail="ê¸°ê¸° ë“±ë¡ì´ í•„ìš”í•©ë‹ˆë‹¤.")

    # ê³µì§€ í™•ì¸
    res_notice = await db.execute(select(Notice).filter(Notice.id == notice_id))
    notice = res_notice.scalars().first()
    if not notice:
        raise HTTPException(status_code=404, detail="ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")

    # ìŠ¤í¬ë© ì—¬ë¶€ í™•ì¸
    stmt_scrap = select(Scrap).filter(
        Scrap.device_id == device.id, 
        Scrap.notice_id == notice_id
    )
    res_scrap = await db.execute(stmt_scrap)
    existing_scrap = res_scrap.scalars().first()

    try:
        if existing_scrap:
            # ì‚­ì œ ì‹œ delete(...) ëŒ€ì‹  ê°ì²´ë¥¼ db.delete()ë¡œ ë„˜ê¸°ê±°ë‚˜ stmt ì‹¤í–‰
            await db.delete(existing_scrap)
            await db.commit()
            return {"status": "removed", "message": "ìŠ¤í¬ë© ì·¨ì†Œë¨"}
        else:
            new_scrap = Scrap(device_id=device.id, notice_id=notice_id)
            db.add(new_scrap)
            await db.commit()
            return {"status": "added", "message": "ìŠ¤í¬ë© ì €ì¥ë¨"}
    except Exception as e:
        await db.rollback()
        logger.error(f"ìŠ¤í¬ë© ì—ëŸ¬: {e}")
        raise HTTPException(status_code=500, detail="DB Error")

@router.get("/scraps", response_model=List[NoticeListResponse])
async def get_my_scraps(token: str, db: AsyncSession = Depends(get_db)):
    res_device = await db.execute(select(Device).filter(Device.token == token))
    device = res_device.scalars().first()
    
    if not device:
        return []

    # Join ì¿¼ë¦¬ (2.0 Style)
    stmt = (
        select(Notice)
        .join(Scrap, Notice.id == Scrap.notice_id)
        .filter(Scrap.device_id == device.id)
        .order_by(Scrap.created_at.desc())
    )
    result = await db.execute(stmt)
    scraped_notices = result.scalars().all()

    for notice in scraped_notices:
        notice.is_scraped = True
        
    return scraped_notices